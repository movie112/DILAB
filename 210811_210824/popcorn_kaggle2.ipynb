{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "popcorn_kaggle2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VpVsc46qT8mvBZmRqlYQLHd1vXbjw6ZS",
      "authorship_tag": "ABX9TyP/lxZ1pObjfESs6PayxCIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/210811_210824/popcorn_kaggle2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfp6ex9z2lmb"
      },
      "source": [
        "# Bag of Words Meets Bags of Popcorn\n",
        "### Use Google's Word2Vec for movie reviews\n",
        "<https://www.kaggle.com/c/word2vec-nlp-tutorial/overview>\n",
        "- 영화 사이트 IMDB 영화 리뷰 데이터셋 이용\n",
        "- 리뷰 긍정 1 부정 0 으로 표시\n",
        "- files\n",
        "  - labeldTrainData: 라벨링된 트레이닝 셋. 내용은 tab으로 구분되며 id, sentiment, review로 구성\n",
        "  - testData: id, review 구성. 모델을 트레이닝할 때 테스트셋으로 사용하라는 것이 아니라 최종적으로 이 데이터셋을 이용하여 판단한다는 의미로 보임(unlabeled)\n",
        "  - unlabeledTrainData: 라벨링되지 않은 트레이닝 셋\n",
        "  - sampleSubmission: 제출 포맷\n",
        "- word2vec, randomforest\n",
        "- 참고\n",
        "  - <https://nlp.gitbook.io/book/tf-with-kaggle/untitled/bag-of-words-meets-bags-of-popcorn/word2vec>\n",
        "  - <https://www.kaggle.com/harshitmakkar/nlp-word2vec>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Bvm6sD24Zp"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbB2sDUmtmzV",
        "outputId": "95bcd1d2-e0e3-4d1f-a946-57a4b336679b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW35MhEWzRTK"
      },
      "source": [
        "## 데이터\n",
        "- Word2Vec은 unlabeled 데이터를 사용해서 학습할 수 있으므로, 이제 50,000개의 리뷰를 추가적으로 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LjBaS-h2N2a",
        "outputId": "af115d95-91e3-4be0-fbcc-e26539e9f4d6"
      },
      "source": [
        "# Read data from files\n",
        "train = pd.read_csv( \"/content/drive/MyDrive/movie-review/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "test = pd.read_csv( \"/content/drive/MyDrive/movie-review/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "unlabeled_train = pd.read_csv( \"/content/drive/MyDrive/movie-review/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "\n",
        "# Verify the number of reviews that were read (100,000 in total)\n",
        "print(\"Read {0} labeled train reviews, {1} labeled test reviews, and {2} unlabeled reviews\\n\"\\\n",
        "      .format(train[\"review\"].size,  test[\"review\"].size, unlabeled_train[\"review\"].size ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Y16-9dK-2ctK",
        "outputId": "0226a849-d678-4ec7-c12d-a472de60f848"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "iCJbLoH0KNK7",
        "outputId": "e1cf788a-65cb-4fb3-bcce-6f4a231c8010"
      },
      "source": [
        "train['review'][0][:700]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely lik'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NQkMC6WKl3A"
      },
      "source": [
        "##### 흔히 쓰이는 단어\n",
        "- HTML 태그가 흔히 쓰인다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "DYmANUX3Klhd",
        "outputId": "85a12b1e-ce1b-4d19-d510-df636a43d8a4"
      },
      "source": [
        "corpus=[] \n",
        "for x in train['review'].str.split():\n",
        "  for i in x:\n",
        "    corpus.append(i)\n",
        "counter = Counter(corpus)\n",
        "\n",
        "most = counter.most_common()\n",
        "print(most[0], most[1][0][0]) # 빈도, 문자열만 추출\n",
        "\n",
        "x =[]\n",
        "y =[]\n",
        "\n",
        "for word, count in most[:40]:\n",
        "  if(word not in set(stopwords.words('english'))):   \n",
        "    x.append(word)\n",
        "    y.append(count)\n",
        "\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('the', 286998) a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4b3154d690>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk0X_FqM4Kap"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4NSlumB3FRZ"
      },
      "source": [
        "#### 전처리 함수\n",
        "- Word2Vec는 문장의 문맥(context)을 고려하여 하이퀄리티 단어 벡터를 생성하기 때문에, stop word를 제거하는 것이 안 좋을 수 있다. 따라서 아래 함수에서 stop word 제거를 옵셔널하게 바꾸었다. 마찬가지의 이유로 숫자도 남겨두는 것이 더 좋을 수도 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2c0VcYg2fby"
      },
      "source": [
        "# Import various modules for string cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    #  \n",
        "    # 2. Remove non-letters -> 공백\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    #\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 5. Return a list of words\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXduq-Hj3mtZ"
      },
      "source": [
        "#### Tokenization function\n",
        "- input format을 맞추어야 한다. Word2Vec은 각 문장이 단어 list로 구성된 list를 input으로 받는다. 즉, input format은 2중 list 구조\n",
        "- 문단(paragraph)을 문장(sentence)으로 나누는 것은 간단한 작업이 아니다. 자연어(natural language)에는 수많은 변수들이 존재한다. 영어 문장은 “?”, “!”, “”“, “.” 등 다양한 문자로 끝날 수 있고, 띄어쓰기나 대문자는 별로 신뢰할만한 기준이 되지 못한다. 이러한 이유로, 문장 분리를 위해 NLTK의 punkt tokenizer를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ej96xf3-cR"
      },
      "source": [
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words, so this returns a list of lists\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bekhAOUp3-D6"
      },
      "source": [
        "#### data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcGmXWqP4cc2",
        "outputId": "634eec56-41c2-46cd-f1ce-5552f78e4156"
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print(\"Parsing sentences from training set\")\n",
        "for i, review in enumerate(train[\"review\"]):\n",
        "    if (i+1) % 1000 == 0:\n",
        "        print(\"[training set] {} of {}\".format(i+1, train[\"review\"].size))\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "\n",
        "print(\"Parsing sentences from unlabeled set\")\n",
        "for i, review in enumerate(unlabeled_train[\"review\"]):\n",
        "    if (i+1) % 1000 == 0:\n",
        "        print(\"[unlabeled set] {} of {}\".format(i+1, unlabeled_train[\"review\"].size))\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[training set] 1000 of 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[training set] 2000 of 25000\n",
            "[training set] 3000 of 25000\n",
            "[training set] 4000 of 25000\n",
            "[training set] 5000 of 25000\n",
            "[training set] 6000 of 25000\n",
            "[training set] 7000 of 25000\n",
            "[training set] 8000 of 25000\n",
            "[training set] 9000 of 25000\n",
            "[training set] 10000 of 25000\n",
            "[training set] 11000 of 25000\n",
            "[training set] 12000 of 25000\n",
            "[training set] 13000 of 25000\n",
            "[training set] 14000 of 25000\n",
            "[training set] 15000 of 25000\n",
            "[training set] 16000 of 25000\n",
            "[training set] 17000 of 25000\n",
            "[training set] 18000 of 25000\n",
            "[training set] 19000 of 25000\n",
            "[training set] 20000 of 25000\n",
            "[training set] 21000 of 25000\n",
            "[training set] 22000 of 25000\n",
            "[training set] 23000 of 25000\n",
            "[training set] 24000 of 25000\n",
            "[training set] 25000 of 25000\n",
            "Parsing sentences from unlabeled set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 1000 of 50000\n",
            "[unlabeled set] 2000 of 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 3000 of 50000\n",
            "[unlabeled set] 4000 of 50000\n",
            "[unlabeled set] 5000 of 50000\n",
            "[unlabeled set] 6000 of 50000\n",
            "[unlabeled set] 7000 of 50000\n",
            "[unlabeled set] 8000 of 50000\n",
            "[unlabeled set] 9000 of 50000\n",
            "[unlabeled set] 10000 of 50000\n",
            "[unlabeled set] 11000 of 50000\n",
            "[unlabeled set] 12000 of 50000\n",
            "[unlabeled set] 13000 of 50000\n",
            "[unlabeled set] 14000 of 50000\n",
            "[unlabeled set] 15000 of 50000\n",
            "[unlabeled set] 16000 of 50000\n",
            "[unlabeled set] 17000 of 50000\n",
            "[unlabeled set] 18000 of 50000\n",
            "[unlabeled set] 19000 of 50000\n",
            "[unlabeled set] 20000 of 50000\n",
            "[unlabeled set] 21000 of 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 22000 of 50000\n",
            "[unlabeled set] 23000 of 50000\n",
            "[unlabeled set] 24000 of 50000\n",
            "[unlabeled set] 25000 of 50000\n",
            "[unlabeled set] 26000 of 50000\n",
            "[unlabeled set] 27000 of 50000\n",
            "[unlabeled set] 28000 of 50000\n",
            "[unlabeled set] 29000 of 50000\n",
            "[unlabeled set] 30000 of 50000\n",
            "[unlabeled set] 31000 of 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 32000 of 50000\n",
            "[unlabeled set] 33000 of 50000\n",
            "[unlabeled set] 34000 of 50000\n",
            "[unlabeled set] 35000 of 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 36000 of 50000\n",
            "[unlabeled set] 37000 of 50000\n",
            "[unlabeled set] 38000 of 50000\n",
            "[unlabeled set] 39000 of 50000\n",
            "[unlabeled set] 40000 of 50000\n",
            "[unlabeled set] 41000 of 50000\n",
            "[unlabeled set] 42000 of 50000\n",
            "[unlabeled set] 43000 of 50000\n",
            "[unlabeled set] 44000 of 50000\n",
            "[unlabeled set] 45000 of 50000\n",
            "[unlabeled set] 46000 of 50000\n",
            "[unlabeled set] 47000 of 50000\n",
            "[unlabeled set] 48000 of 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[unlabeled set] 49000 of 50000\n",
            "[unlabeled set] 50000 of 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MKHSvuK4iDZ",
        "outputId": "b546febb-ce46-450f-87ba-595c1dd83948"
      },
      "source": [
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwjd83PR4hxJ"
      },
      "source": [
        "## Training and Saving Model\n",
        "- 모델을 학습할 준비가 되었다. 실행시간과 최종 모델의 정확도에 영향을 끼치는 파라메터들의 값을 선택해야 한다.\n",
        "> - Architecture: 아키텍처 옵션은 skip-gram (default) 와 continuous bag of words가 있다. skip-gram이 미세하게 느리지만 더 좋은 결과를 보여준다.\n",
        "Training algorithm: hierarchical softmax (default) 와 negative sampling이 있다. 여기서는, 디폴트가 좋다.\n",
        "> - Downsampling of frequent words: 구글 도큐먼트에서 .00001에서 .001 사이의 값을 추천한다. 여기서는, 0.001에 가까운 값이 좋아 보인다.\n",
        "> - Word vector dimensionality: 많은 특성(feature)은 더 많은 학습시간을 요구하지만, 보통 더 좋은 결과를 낸다(항상 그런것은 아니다). 수십에서 수백 정도가 적당한 값이다; 우리는 300개의 특성을 사용한다.;\n",
        "> - Context / window size: word2vec은 어떤 단어 주변의 단어들, 즉 문맥을 고려해서 해당 단어의 의미를 파악한다. 이 때 얼마나 많은 단어를 고려해야 할까? 10 정도가 hierarchical softmax에 적당하다. 이 값도 어느정도까지는 높을수록 좋다.\n",
        "> - Worker threads: 패러렐 쓰레드의 수. 컴퓨터마다 다르겠지만, 일반적으로 4~6 정도가 적당하다.\n",
        "> - Minimum word count: meaningful word를 규정하는 최소 word count. 이 수치 미만으로 등장하는 단어는 무시한다. 10에서 100 사이의 값이 적당하다. 우리의 경우, 각 영화가 30번씩 등장하므로, 영화 제목에 너무 많은 의미 부여를 피하기 위해 minimum word count를 40으로 설정하였다. 그 결과로 vocabulary size는 약 15,000개의 단어다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKXKGSMQIB6s"
      },
      "source": [
        "맨 앞, sentences는 학습시킬 문장을 입력합니다.   \n",
        "workers : 실행할 병렬 프로세스의 수, 코어수, 주로 4-6사이 지정   \n",
        "size : 각 단어에 대한 임베딩 된 벡터차원 정의, size=2라면 한 문장의 벡터는 [-0.1248574, 0.255778]와 같은 형태를 가지게 된다.   \n",
        "min_count : 단어에 대한 최소 빈도수. min_count=5라면 빈도수 5 이하 무시   \n",
        "window : 문맥 윈도우 수, 양쪽으로 몇 개의 단어까지 고려해서 의미를 파악할 것인지 지정하는 것   \n",
        "sample : 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정하는 것, 보통 0.001이 좋은 성능을 낸다고 한다.   \n",
        "sg : 1이면 skip-gram 방법을 사용하고, 0이면 CBOW 방법을 사용한다.   \n",
        "iter : epoch와 같은 뜻으로 학습 반복 횟수를 지정한다.\n",
        "<https://ebbnflow.tistory.com/153>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KNsZ4yd5Kph"
      },
      "source": [
        "# Import the built-in logging module and configure it so that Word2Vec\n",
        "# creates nice output messages\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "print(\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
        "            size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "# If you don't plan to train the model any further, calling\n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and\n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZCW1GVM6vO5"
      },
      "source": [
        "## Model Result\n",
        "- “doesnt_match” 함수는 주어진 단어 셋(set) 중에서 가장 비슷하지 않은 단어를 추정한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "RZHe_QpV6ykH",
        "outputId": "c666de3f-abb1-4147-f5f0-d2c7f9b6f757"
      },
      "source": [
        "model.doesnt_match(\"man women child kitchen\".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kitchen'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DCcDN_A7cCA"
      },
      "source": [
        "우리 모델이 이 의미 차이를 구분해낸다! man, women, children이 비슷하고 kitchen과는 다르다는 것을 알고 있다. 이제 도시와 나라같은 미묘한 차이도 구분하는지 확인해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "1UzgKV0L7crl",
        "outputId": "d946288a-aa84-4fe6-9b17-54008e1ea474"
      },
      "source": [
        "model.doesnt_match(\"france england germany berlin\".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'berlin'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDJjCuFh7jlf",
        "outputId": "009b0b8f-d2fc-4233-e44a-6443b71cca3a"
      },
      "source": [
        "model.most_similar(\"man\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.6267052888870239),\n",
              " ('lady', 0.5899450182914734),\n",
              " ('lad', 0.5555330514907837),\n",
              " ('monk', 0.5424312353134155),\n",
              " ('chap', 0.533638596534729),\n",
              " ('guy', 0.5276709794998169),\n",
              " ('soldier', 0.5261393785476685),\n",
              " ('millionaire', 0.5131581425666809),\n",
              " ('men', 0.5098788738250732),\n",
              " ('businessman', 0.5097929239273071)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B91eem1d7sRA"
      },
      "source": [
        "또는, 감정분석을 위해서는 이런 걸 찾아보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_bzfHhK7tig",
        "outputId": "9262c8f8-0ff2-433a-a799-8347b4cc4e51"
      },
      "source": [
        "model.most_similar(\"awful\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.7765267491340637),\n",
              " ('horrible', 0.7447943687438965),\n",
              " ('atrocious', 0.7402299046516418),\n",
              " ('abysmal', 0.7181999683380127),\n",
              " ('dreadful', 0.6820523738861084),\n",
              " ('horrendous', 0.6772993803024292),\n",
              " ('horrid', 0.6707334518432617),\n",
              " ('appalling', 0.6601128578186035),\n",
              " ('lousy', 0.6226420402526855),\n",
              " ('laughable', 0.6196815371513367)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjchf-ZVAKN_"
      },
      "source": [
        "IMDB 데이터셋의 한 가지 문제는 리뷰의 길이가 변한다는 것이다. 우리는 여기서 각각의 단어 벡터들을 각 리뷰를 나타내는 동일한 크기의 특성 셋으로 나타내어야 한다.\n",
        "모든 단어들이 전부 300차원 벡터이므로, 간단하게 각 리뷰의 벡터들을 평균 내는 방법을 사용할 수 있다 (이를 위해 stop word를 제거했다. 이러한 경우에 stop word는 노이즈가 된다).\n",
        "아래 코드들은 벡터들을 평균내는 함수다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1jaa1KkAlSP"
      },
      "source": [
        "def makeFeatureVec(words, model, num_features):\n",
        "    # Function to average all of the word vectors in a given\n",
        "    # paragraph\n",
        "    #\n",
        "    # Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    #\n",
        "    nwords = 0.\n",
        "    #\n",
        "    # Index2word is a list that contains the names of the words in\n",
        "    # the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    #\n",
        "    # Loop over each word in the review and, if it is in the model's\n",
        "    # vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    #\n",
        "    # Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n",
        "    #\n",
        "    # Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n",
        "\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    # Given a set of reviews (each one a list of words), calculate\n",
        "    # the average feature vector for each one and return a 2D numpy array\n",
        "    #\n",
        "    # Initialize a counter\n",
        "    counter = 0.\n",
        "    #\n",
        "    # Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    #\n",
        "    # Loop through the reviews\n",
        "    for review in reviews:\n",
        "       #\n",
        "       # Print a status message every 1000th review\n",
        "       if counter%1000. == 0.:\n",
        "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
        "       #\n",
        "       # Call the function (defined above) that makes average feature vectors\n",
        "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \\\n",
        "           num_features)\n",
        "       #\n",
        "       # Increment the counter\n",
        "       counter = counter + 1.\n",
        "    return reviewFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv2rpGImAnoe"
      },
      "source": [
        "이제 이 함수로 각 리뷰에 대한 평균 벡터를 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRjb1ztJAuJg",
        "outputId": "c3525b3e-40ef-4a2a-f8bf-ea7837d3b41c"
      },
      "source": [
        "clean_train_reviews = []\n",
        "for c, review in enumerate(train[\"review\"]):\n",
        "    if c%1000. == 0.:\n",
        "        print(\"Training set {} of {}\".format(c, train.shape[0]))\n",
        "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
        "\n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "\n",
        "print(\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for c, review in enumerate(test[\"review\"]):\n",
        "    if c%1000. == 0.:\n",
        "        print(\"Test set {} of {}\".format(c, test.shape[0]))\n",
        "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
        "\n",
        "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set 0 of 25000\n",
            "Training set 1000 of 25000\n",
            "Training set 2000 of 25000\n",
            "Training set 3000 of 25000\n",
            "Training set 4000 of 25000\n",
            "Training set 5000 of 25000\n",
            "Training set 6000 of 25000\n",
            "Training set 7000 of 25000\n",
            "Training set 8000 of 25000\n",
            "Training set 9000 of 25000\n",
            "Training set 10000 of 25000\n",
            "Training set 11000 of 25000\n",
            "Training set 12000 of 25000\n",
            "Training set 13000 of 25000\n",
            "Training set 14000 of 25000\n",
            "Training set 15000 of 25000\n",
            "Training set 16000 of 25000\n",
            "Training set 17000 of 25000\n",
            "Training set 18000 of 25000\n",
            "Training set 19000 of 25000\n",
            "Training set 20000 of 25000\n",
            "Training set 21000 of 25000\n",
            "Training set 22000 of 25000\n",
            "Training set 23000 of 25000\n",
            "Training set 24000 of 25000\n",
            "Review 0 of 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Review 1000 of 25000\n",
            "Review 2000 of 25000\n",
            "Review 3000 of 25000\n",
            "Review 4000 of 25000\n",
            "Review 5000 of 25000\n",
            "Review 6000 of 25000\n",
            "Review 7000 of 25000\n",
            "Review 8000 of 25000\n",
            "Review 9000 of 25000\n",
            "Review 10000 of 25000\n",
            "Review 11000 of 25000\n",
            "Review 12000 of 25000\n",
            "Review 13000 of 25000\n",
            "Review 14000 of 25000\n",
            "Review 15000 of 25000\n",
            "Review 16000 of 25000\n",
            "Review 17000 of 25000\n",
            "Review 18000 of 25000\n",
            "Review 19000 of 25000\n",
            "Review 20000 of 25000\n",
            "Review 21000 of 25000\n",
            "Review 22000 of 25000\n",
            "Review 23000 of 25000\n",
            "Review 24000 of 25000\n",
            "Creating average feature vecs for test reviews\n",
            "Test set 0 of 25000\n",
            "Test set 1000 of 25000\n",
            "Test set 2000 of 25000\n",
            "Test set 3000 of 25000\n",
            "Test set 4000 of 25000\n",
            "Test set 5000 of 25000\n",
            "Test set 6000 of 25000\n",
            "Test set 7000 of 25000\n",
            "Test set 8000 of 25000\n",
            "Test set 9000 of 25000\n",
            "Test set 10000 of 25000\n",
            "Test set 11000 of 25000\n",
            "Test set 12000 of 25000\n",
            "Test set 13000 of 25000\n",
            "Test set 14000 of 25000\n",
            "Test set 15000 of 25000\n",
            "Test set 16000 of 25000\n",
            "Test set 17000 of 25000\n",
            "Test set 18000 of 25000\n",
            "Test set 19000 of 25000\n",
            "Test set 20000 of 25000\n",
            "Test set 21000 of 25000\n",
            "Test set 22000 of 25000\n",
            "Test set 23000 of 25000\n",
            "Test set 24000 of 25000\n",
            "Review 0 of 25000\n",
            "Review 1000 of 25000\n",
            "Review 2000 of 25000\n",
            "Review 3000 of 25000\n",
            "Review 4000 of 25000\n",
            "Review 5000 of 25000\n",
            "Review 6000 of 25000\n",
            "Review 7000 of 25000\n",
            "Review 8000 of 25000\n",
            "Review 9000 of 25000\n",
            "Review 10000 of 25000\n",
            "Review 11000 of 25000\n",
            "Review 12000 of 25000\n",
            "Review 13000 of 25000\n",
            "Review 14000 of 25000\n",
            "Review 15000 of 25000\n",
            "Review 16000 of 25000\n",
            "Review 17000 of 25000\n",
            "Review 18000 of 25000\n",
            "Review 19000 of 25000\n",
            "Review 20000 of 25000\n",
            "Review 21000 of 25000\n",
            "Review 22000 of 25000\n",
            "Review 23000 of 25000\n",
            "Review 24000 of 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVBLGWei8vVA"
      },
      "source": [
        "주석으로 설명이 적혀 있지만, 간단히 설명하자면 makeFeatureVec은 리뷰 파라그래프를 받아서 각 단어들에 대해 model이 포함하는 단어인지 검사하여 평균을 구한다. 즉, 특정 리뷰에 대해 특성 벡터를 구하는 함수이고 getAvgFeatureVecs는 모든 리뷰에 대해 makeFeatureVec함수를 적용하여 특성 벡터 리스트를 구하는 함수다.\n",
        "자, 그럼 이제 각 리뷰들의 특성 벡터를 추출하였으니 이 값으로 머신러닝 알고리즘을 돌릴 수 있다. Bag of Words에서 했던 것처럼 랜덤 포레스트를 적용해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HglQ_b3A7hf",
        "outputId": "d6e147eb-32ba-4343-87de-bac49bdaffa3"
      },
      "source": [
        "# Fit a random forest to the training data, using 100 trees\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier( n_estimators = 100 )\n",
        "\n",
        "print(\"Fitting a random forest to labeled training data...\")\n",
        "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
        "\n",
        "# Test & extract results\n",
        "result = forest.predict( testDataVecs )\n",
        "\n",
        "# Write the test results\n",
        "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
        "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting a random forest to labeled training data...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}