{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPWikidocs_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnWSiJCRFFB7NRo/jV+6m4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/NLPWikidocs_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-W1zmuX5lik"
      },
      "source": [
        "# 딥러닝을 이용한 자연어처리 입문\n",
        "## 2. 텍스트 전처리(text preprocessing)\n",
        "- 01) 토큰화(Tokenization)\n",
        "- 02) 정제, 정규화(Cleaning, Normalization)\n",
        "- 03) 어간, 표제어추출(Stemming, Lemmatization)\n",
        "- 04) 불용어(Stopword)\n",
        "- 05) 정규 표현식(Regular Expression)\n",
        "- 06) 정수 인코딩(Integer Encoding)\n",
        "- 07) 패딩(Padding)\n",
        "- 08) 원-핫 인코딩(One-Hot Encoding)\n",
        "- 09) 데이터의 분리(Dplitting Data)\n",
        "- 10) 한국어 전처리 패키지(Text preprocessing tools for Kerean text)\n",
        "<https://wikidocs.net/21698>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB-NEDnRYSHj"
      },
      "source": [
        "---\n",
        "\n",
        "### 01) 토큰화(tokenization)\n",
        "데이터가 전처리되지 않은 상태라면, 사용하고자하는 용도에 맞게 토큰화(tokenization), 정제(cleaning), 정규화(normalization)을 진행한다.   \n",
        "- 토큰화   \n",
        "corpus에서 토큰 단위로 나누는 작업\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4RijI1FYYdS"
      },
      "source": [
        "#### 1. 단어 토큰화(word tokenization)\n",
        "토큰의 기준: 단어(word)   \n",
        "- 입력: Time is an illusion. Lunchtime double so!\n",
        "- 구두점을 제외시킨 토큰화 작업의 결과\n",
        "- 출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGRMXnhbYgJx"
      },
      "source": [
        "#### 2. 토큰화 중 생기는 선택의 순간\n",
        "-  Don't와 Jone's는 어떻게 토큰화할 수 있을까?\n",
        "-  NLTK: 영어 corpus를 토큰화하기 위한 도구들 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UD9joeg5ijr",
        "outputId": "158eff20-2d83-49ed-c28f-3e2360f2775a"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "nltk.download('punkt')\n",
        "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVaAjsc9GzAA"
      },
      "source": [
        "- word_tokenize: Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82DWZiatG5q3",
        "outputId": "277b8d13-e13f-4d4c-923a-56d7bbe7f1e1"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euEAe-xTG9mQ"
      },
      "source": [
        "- WordPunctTokenizer: 구두점을 별도로 분류, word_tokenize와는 달리 Don't를 Don과 '와 t로 분리하였으며, Jone's를 Jone과 '와 s로 분리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDAb5QIKHOGj"
      },
      "source": [
        "- keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAy3AqvKHP8S",
        "outputId": "2fe2a10f-e7e8-4e6f-c631-642f8cd8490f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GNjVCeWHVsr"
      },
      "source": [
        "- text_to_word_sequence: 기본적으로 모든 알파벳을 소문자로 바꾸면서 구두점 제거. 아포스트로피는 보존"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEupyi8HgY7"
      },
      "source": [
        "#### 3. 토큰화에서 고려해야할 사항\n",
        "- 구두점이나 특수 문자를 단순 제외X\n",
        "  - 마침표(.)와 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로\n",
        "  - 단어 자체에서 구두점을 갖고 있는 경우 /  Ph.D나 $45.55, 01/02/06\n",
        "- 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
        "  - 사용 용도에 따라서, 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, 토큰화 작업은 저러한 단어를 하나로 인식할 수 있는 능력 가져야 함\n",
        "  - what're, I'm / re = 접어(clitic)\n",
        "  - New York, rock 'n' roll\n",
        "- 표준 토큰화 예제\n",
        "  -  Penn Treebank Tokenization의 규칙\n",
        "    - 규칙1) 하이푼으로 구성된 단어는 하나로 유지\n",
        "    - 규칙2) doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuSTLPa9REji",
        "outputId": "20b8fb7c-1b6f-4d2b-8bf3-5f942d01f21d"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l7pjT1TRLsy"
      },
      "source": [
        "- home-based는 하나의 토큰으로 취급하고 있으며, dosen't의 경우 does와 n't는 분리되었음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP2D8BQtRM4S"
      },
      "source": [
        "#### 4. 문장 토큰화(Sentence tokenization)\n",
        "- corpus 내 토큰의 단위가 문장(sentence), 문장 분류(sentence segmentation)라고도 부름\n",
        "- 정제되지 않은 corpus라면 문장 단위로 구분되어있지 않을 가능성이 높다.\n",
        "- !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 마침표는 문장의 끝이 아니라도 등장 가능\n",
        "- NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GugQX-mR39i",
        "outputId": "1d51321d-e3ad-4935-9117-d952c12106be"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa7ttcqR8iZ"
      },
      "source": [
        "- 마침표 여러 번 등장하는 경우"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueY-whHZSBwJ",
        "outputId": "340782ef-fe6e-4178-f8c1-99e1c8762aff"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe9xCdFhSFNh"
      },
      "source": [
        "- 단순히 마침표를 구분자로 하지 않았기 때문에 Ph.D.를 문장 내의 단어로 인식\n",
        "- 한국어에 대한 문장 토큰화 도구는 박상길님이 개발한 KSS(Korean Sentence Splitter)를 추천"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvX05V9-SVB5",
        "outputId": "75c2216e-3363-4bbb-8fcc-438511760178"
      },
      "source": [
        "pip install kss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-2.5.1-py3-none-any.whl (65 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 2.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: kss\n",
            "Successfully installed kss-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8wdr_dOSYQB",
        "outputId": "064da5a5-c6f0-4b2d-b3c3-d9bf85af3c4e"
      },
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OvwvHr9SaZh"
      },
      "source": [
        "#### 이진 분류기(Binary Classifier)\n",
        "- 문장 토큰화에서 마침표 처리를 위해 입력에 따라 두 개의 클래스로 분류\n",
        "  - 클래스1) 마침표(.)가 단어의 일부분인 경우 (약어abbreivation)\n",
        "  - 클래스2) 마침표가 문장의 구분자(boundary)일 경유\n",
        "- 임의로 정한 여러가지 규칙을 코딩한 함수일 수도 있으며, 머신러닝을 통해 이진 분류기를 구현하기도 함\n",
        "- 마침표가 어떤 클래스에 속하는지 결정하기 위해 어떤 마침표가 주로 약어(abbreviation)으로 쓰이는 지 알아야 함, \n",
        "  - 약어 사전이 유용 [약어사전의 예](https://public.oed.com/how-to-use-the-oed/abbreviations/)\n",
        "\n",
        "-  문장 토큰화를 수행하는 오픈 소스로: NLTK, OpenNLP, 스탠포드 CoreNLP, splitta, LingPipe 등\n",
        "- [문장 토큰화 규칙을 짤 때, 발생할 수 있는 여러가지 예외사항을 다룬 참고 자료](https://www.grammarly.com/blog/engineering/how-to-split-sentences/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ7HLLFVYx-q"
      },
      "source": [
        "#### 6. 한국어에서의 토큰화의 어려움\n",
        "- 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 즉, 어절 토큰화는 한국어 NLP에서 지양   \n",
        "##### 1) 한국어는 교착어이다\n",
        "- 교착어: 조사, 어미 등을 붙여서 말을 만드는 언어\n",
        "- 조사가 글자 뒤에 띄어쓰기 없이 붙는다. 처리를 하다보면 같은 단어임에도 다른 조사로 인해 다른 단어로 인식이 되면 자연어 처리가 힘들어진다. 한국어 NLP는 조사를 분리해야 함\n",
        "\n",
        "- 형태소(morpheme): 가장 작은 말의 단위\n",
        "  - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등\n",
        "  - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간\n",
        "- 형태소 토큰화를 수행해야 함\n",
        "\n",
        "##### 2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
        "\n",
        "#### 7. 품사 태깅(Part-of-speech tagging)\n",
        "- 단어는 표기는 같지만, 품사에 따라서 단어의 의미가 다를 수 있다.\n",
        "- ex) fly: 날다, 파리\n",
        "- 단어의 의미를 파악하기 위해 품사 태깅이 지표가 될 수 있다.\n",
        "\n",
        "#### 8. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
        "- NLTK에서는 영어 코퍼스에 품사 태깅 기능을 지원, Penn Treebank POS Tags라는 기준 사용\n",
        "- PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6mj6PE5VkkK",
        "outputId": "47c228c5-1a84-48ae-df76-ea5c7241d89f"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTkkz5qTVmfi",
        "outputId": "e31919de-f86b-48a6-f8fb-f73e89e81671"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "x=word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTAl0xCjV5N5"
      },
      "source": [
        "- 한국어 자연어 처리를 위해서 파이썬 패키지 KoNLPy(코엔엘파이)사용 \n",
        "- 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma) 존재\n",
        "- 향태소 분석기마다 결과가 달라서 어떤 것이 적절한지 판단하고 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmAtONtwWK5Q"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P12GkbLXWh_o"
      },
      "source": [
        "---\n",
        "### 02) 정제(Cleaning) and 정규화(Normalization)\n",
        "- 정제(cleaning) : 갖고 있는 corpus로부터 노이즈 데이터를 제거\n",
        "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
        "- 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 합니다. \n",
        "\n",
        "#### 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
        "- ex) USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화, 정규화를 하면, US를 찾아도 USA 함께 찾을 수 있다.\n",
        "#### 2. 대, 소문자 통합\n",
        "- 대문자와 소문자를 무작정 통합X \n",
        "  - 대문자와 소문자가 구분되어야 하는 경우도 있음\n",
        "  - ex) 미국을 뜻하는  US / 우리를 뜻하는 us,  회사 이름(General Motors)나, 사람 이름(Bush) 등은 대문자로 유지되는 것이 옳다.\n",
        "#### 불필요한 단어의 제거(Removing Unnecessary Words)\n",
        "- noise data: 자연어가 아니면서 아무 의미 없는 글자들(ex.특수문자), 분석 목적에 맞지 않은 단어들\n",
        "- 불필요 단어들을 제거하는 방법: 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법\n",
        "  - (1) 등장 빈도가 적은 단어(Removing Rare words)\n",
        "  - (2) 길이가 짧은 단어(Removing words with a very short length)\n",
        "    - 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함도 있음\n",
        "#### 4. 정규 표현식(Regular Expression)\n",
        "- corpus 내 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로서 매우 유용\n",
        "-  자세한 내용은 다른 챕터에서\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8ym0mguYmgp",
        "outputId": "fce55205-d505-4e37-dc67-fc90a6939e29"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rXrfo_HZZMR"
      },
      "source": [
        "---\n",
        "\n",
        "### 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
        "- corpus의 단어 개수 줄이는 기법, 서로 다른 단어들이지만, 하나의 단어로 일반화시켜 문서 내의 단어 수를 줄인다.\n",
        "- 단어의 빈도수를 기반으로 문제를 풀고자하는 BOW(Bag of Words)표현은 사용하는 문제에서 주로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9kjs7v3ZNkW"
      },
      "source": [
        "#### 1. 표제어 추출(Lemmatization)\n",
        "- 표제어(Lemma): '표제어', '기본 사전형 단어' 정도의 의미\n",
        "- 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단\n",
        "   - ex. \"am, are, is\" 스펠링 다르지만 뿌리 단어는 be, 이 단어들의 표제어는 be\n",
        "- 형태소\n",
        "  - 어간(stem): 단어의 의미를 담고 있는 단어의 핵심 부분\n",
        "  - 접사(affix): 단어에 추가적인 의미를 주는 부분\n",
        "\n",
        "- 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것, \n",
        "- 형태학적 파싱: 이 두 가지 구성 요소를 분리하는 작업\n",
        "  - ex) cats -> cat(어간)와 -s(접사) 분리\n",
        "- NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UKWJkSMbHQ5",
        "outputId": "e2107d92-d1d0-496c-ee05-7e83eef37819"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjdwDV2bbXTp"
      },
      "source": [
        "-  표제어 추출은 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상\n",
        "- 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력\n",
        "- 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문\n",
        "- WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QGcaAcZdbXAq",
        "outputId": "31b0d38d-38a8-4f1d-eb7c-eb961fed01f8"
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sqdiozCbn37"
      },
      "source": [
        "- 표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. (POS 태그를 보존)\n",
        "- 어간 추출을 수행한 결과는 품사 정보가 보존되지 않음, 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많다.\n",
        "\n",
        "#### 2. 어간 추출(Stemming)\n",
        "- 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업\n",
        "- 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음\n",
        "- ex.포터 알고리즘(Porter Algorithm)\n",
        "  - ALIZE → AL, ANCE → 제거, ICAL → IC\n",
        "  - 어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhO61zYAbqL6",
        "outputId": "10da7a41-eba7-4ba3-c161-df913e8d8a4d"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Co9avEycAFJ",
        "outputId": "7d456663-5607-41a2-a8ff-9f9f3ff793e1"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8n39nUmcM54"
      },
      "source": [
        "- ex. 랭커스터 스태머(Lancaster Stemmer) 알고리즘\n",
        "\n",
        "---\n",
        "\n",
        "### 04) 불용어(Stopword) \n",
        "- 불용어(stopword): 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 의미 분석을 하는데 거의 기여하는 바가 없음\n",
        "\n",
        "#### 1. NLTK에서 불용어 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bv1_MjWdFnp",
        "outputId": "fd2149ed-035b-4fa7-bc8c-0024931cf856"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')[:10] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K69As5a0dMSx"
      },
      "source": [
        "#### 2. NLTK를 통해서 불용어 제거하기\n",
        "- is', 'not', 'an'과 같은 단어들이 문장에서 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLk7V1yMdMC5",
        "outputId": "4885fd25-ff48-412c-d9d4-c8fcdf011fd7"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzQYQYiDdXwB"
      },
      "source": [
        "#### 3. 한국어에서 불용어 제거하기\n",
        "- [보편적으로 선택하는 한국어 불용어 리스트](https://www.ranks.nl/stopwords/korean)\n",
        "- [추가 참고 한국어 불용어 리스트](https://bab2min.tistory.com/544)\n",
        "- 코드 내에서 직접 정의하지 않고 txt 파일이나 csv 파일로 수많은 불용어를 정리해놓고, 이를 불러와서 사용하는 방법이 더 좋다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlZDTB5ndtx5"
      },
      "source": [
        "---\n",
        "### 05) 정규 표현식(Regular Expression)\n",
        "- 정규 표현식 모듈 re의 사용 방법과 NLTK를 통한 정규 표현식을 이용한 토큰화에 대해서 알아보도록 하자.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybvewnnZbM5"
      },
      "source": [
        "#### 1. 정규 표현식 문법과 모듈 함수\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36RMrK0AZdMc"
      },
      "source": [
        "##### - 1) 정규 표현식 문법 -\n",
        "특수문자   \n",
        "- 파이썬은 정규 표현식 모듈 re을 지원, 특정 규칙이 있는 텍스트 데이터를 빠르게 정제 가능   \n",
        "- `.` | 임의의 문자 1개 (\\n 제외)   \n",
        "- `?` | 앞 문자 존재 가능성 있음 (0 or 1개)   \n",
        "- `*` | 앞 문자 무한개 가능성 (0개 이상)   \n",
        "- `+` | 앞 문자 한 개 이상 존재 (1개 이상)   \n",
        "- `^` | 뒤로 문자열 시작   \n",
        "- `$` | 앞 문자로 문자열 끝   \n",
        "- `{숫자n}` | n만큼 반복   \n",
        "- `{n1, n2}` | n1이상 n2이하 반복 (?, *, + 으로 대체 가능)   \n",
        "- `{n,}` | n 이상 반복   \n",
        "- `[]` | 대괄호 내 문자들 중 한 개와 매치, [a-z]도 가능   \n",
        "- `[^문자]` | 해당 문자를 제외한 문자 매치\n",
        "- `|` | A|B, A 또는 B\n",
        "\n",
        "역슬래쉬(\\) 사용     \n",
        "- `\\` | 역 슬래쉬 문자 자체\n",
        "- `\\d` | 모든 숫자, `[0-9]`\n",
        "- `\\D` | 숫자를 제외한 모든 문자, `[^0-9]`\n",
        "- `\\s` | 공백, `[ \\t\\n\\r\\f\\v]` \n",
        "- `\\S` | 공백을 제외한 문자, `[^ \\t\\n\\r\\f\\v]`\n",
        "- `\\w` | 문자 또는 숫자, `[a-zA-Z0-9]`\n",
        "- `\\W` | 문자 또는 숫자가 아닌 문자, `[^a-zA-Z0-9]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak1d3JhfZfbS"
      },
      "source": [
        "##### - 2) 정규표현식 모듈 함수 - \n",
        "정규표현식 모듈에서 지원하는 함수     \n",
        "- re.compile(): 정규포현식 컴파일, 찾고자 하는 패턴이 빈번한 경우 미리 컴파일해두면 속도, 편의성 굿\n",
        "- re.search()\t: 문자열 전체에 대해서 정규표현식과 매치되는지를 검색\n",
        "- re.match(): 문자열 처음이 정규표현식과 매치되는지를 검색\n",
        "- re.split(): 정규 표현식을 기준으로 문자열 분리하여 리스트로 리턴\n",
        "- re.findall(): 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아 리스트로 리턴, 없으면 빈 리스트\n",
        "- re.finditer(): 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체 리턴\n",
        "- re.sub(): 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIdL29IX1fPr"
      },
      "source": [
        "#### 2. 정규 표현식 실습\n",
        "- 1) .기호\n",
        "  - .은 한 개의 임의의 문자를 나타냅니다. / \"a.c\": a와 c 사이에 어떤 1개의 문자 올 수 있다.\n",
        "  - serch의 입력에 정규표현식 패턴 \"a.c\"가 존재하는지 확인하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjme8qYdXmR"
      },
      "source": [
        "import re\n",
        "r=re.compile(\"a.c\")\n",
        "r.search(\"kkk\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPVP9bX713AL",
        "outputId": "22665637-8e0a-4628-b61f-39e95e34d214"
      },
      "source": [
        "r.search(\"abc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEk60W2Y2ui1"
      },
      "source": [
        "- 2) ?기호\n",
        "  - ?는 ? 앞의 문자 존재 가능성 있음. \"ab?c\": b가 있다고 할 수도, 없다고 할 수도 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVsONQRM3N5I"
      },
      "source": [
        "r=re.compile(\"ab?c\")\n",
        "r.search(\"abbc\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26NZNx_i3QGz",
        "outputId": "c799b387-0fa9-4d56-8f9a-316ed02558b4"
      },
      "source": [
        "r.search(\"ac\")  # b가 없는 것으로 판단하여 ac를 매치"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7GaHE3F3VhU"
      },
      "source": [
        "- 3) *기호 \n",
        "  - *은 바로 앞의 문자가 0개 이상\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbjWDAt83glL"
      },
      "source": [
        "r=re.compile(\"ab*c\")\n",
        "r.search(\"a\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7VzA9lu3jr7",
        "outputId": "9c4c0cfc-2a62-4f40-ac6a-9e707bed36b7"
      },
      "source": [
        "r.search(\"ac\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga4L_Oup3k7U",
        "outputId": "d2cb7c9f-3326-477f-e2d4-415d684a233f"
      },
      "source": [
        "r.search(\"abbbbc\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXHwWdn3lrj"
      },
      "source": [
        "- 4) +기호\n",
        "  - 앞의 문자가 최소 1개 이상"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Ql-1Wc3rAL"
      },
      "source": [
        "r=re.compile(\"ab+c\")\n",
        "r.search(\"ac\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV7RRLDO3tBS",
        "outputId": "a6c178e3-2b2d-4123-a25b-da6f30c1e487"
      },
      "source": [
        "r.search(\"abbbbc\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vBpksDS3zJK"
      },
      "source": [
        "- 5) ^기호\n",
        "  - ^는 시작되는 글자를 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGIrW5Fv35y7"
      },
      "source": [
        "r=re.compile(\"^a\")\n",
        "r.search(\"bbc\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4q4zPGL36ZC",
        "outputId": "92583452-1e1b-4e07-ef95-e3a44745ffb4"
      },
      "source": [
        "r.search(\"ab\")   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KQQVL23_C7"
      },
      "source": [
        "-  6) {숫자} 기호\n",
        "  - 해당 문자를 숫자만큼 반복\n",
        "  - \"ab{2}c\" a와 c 사이에 b가 존재하면서 b가 2개인 문자열 매치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IGpfv2S4Ndr"
      },
      "source": [
        "r=re.compile(\"ab{2}c\")\n",
        "r.search(\"ac\") # 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"abc\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px8HgM214Ohj",
        "outputId": "9a44d943-86c0-4a4d-b67c-2131ad34cb61"
      },
      "source": [
        "r.search(\"abbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi5hRNNt4NOL"
      },
      "source": [
        "- 7) {숫자1, 숫자2} 기호\n",
        "  - 해당 문자를 숫자1 이상 숫자2 이하만큼 반복"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgEqHwAOCKOS"
      },
      "source": [
        "r=re.compile(\"ab{2,8}c\")\n",
        "r.search(\"ac\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-d_EZFCLQa",
        "outputId": "792c1bf3-5005-4d7f-883f-2f8f60dcd32c"
      },
      "source": [
        "r.search(\"abbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbfoIr9DE1S"
      },
      "source": [
        "- 8) {숫자,} 기호\n",
        "  - 해당 문자를 숫자 이상 만큼 반복"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNl79tRhDYJD"
      },
      "source": [
        "r=re.compile(\"a{2,}bc\")\n",
        "r.search(\"bc\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIRZ8r21Dama",
        "outputId": "b101a11b-bc4e-47c8-e324-c8de33c343e7"
      },
      "source": [
        "r.search(\"aabc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='aabc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbIv6tA8Dcfx"
      },
      "source": [
        "- 9) [ ] 기호\n",
        "  - [ ]안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치\n",
        "  - [0-9]는 숫자 전부"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbI1sFM4DiO6"
      },
      "source": [
        "r=re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\n",
        "r.search(\"zzz\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AunRhCx6FEDa",
        "outputId": "7c38039d-a75c-437f-91bd-8ffff19d997c"
      },
      "source": [
        "r.search(\"baac\")     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='b'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZcnyCYmFJHi"
      },
      "source": [
        "r=re.compile(\"[a-z]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdi5t7yMFK5J",
        "outputId": "b9f89e98-c5c9-4a99-acc6-e63651e6ccb7"
      },
      "source": [
        "r.search(\"aBC\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Xx6ETxFNn4"
      },
      "source": [
        "- 10) [^문자] 기호\n",
        "  - ^ 기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxJGbcpQFV6a"
      },
      "source": [
        "r=re.compile(\"[^abc]\")\n",
        "r.search(\"ab\") # 아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa6OMH3kFash",
        "outputId": "e76257de-31be-4b48-c9c8-53661b2b8bfb"
      },
      "source": [
        "r.search(\"1\")   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='1'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKRda82EFbmy"
      },
      "source": [
        "#### 3. 정규 표현식 모듈 함수 예제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_W8zKtBaGIH"
      },
      "source": [
        "##### (1) re.match() 와 re.search()의 차이\n",
        "- search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKOKnxcyGFXh",
        "outputId": "9edfeba3-58f1-4a43-f098-cbc084c2ae6d"
      },
      "source": [
        "r=re.compile(\"ab.\")\n",
        "r.search(\"kkkabc\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(3, 6), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM2iEOozGJXR"
      },
      "source": [
        "r.match(\"kkkabc\")  #아무런 결과도 출력되지 않는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9xGlBY5GLd5",
        "outputId": "961d497a-1ded-435e-aa71-c2c62bf0f721"
      },
      "source": [
        "r.match(\"abckkk\")  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVAoyfkuGVfp"
      },
      "source": [
        "##### (2) re.split()\n",
        "- 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3AQFpyzIN5K",
        "outputId": "34825752-dc14-4dc5-9618-9a2f9769cc4b"
      },
      "source": [
        "text=\"사과 딸기 수박 메론 바나나\"\n",
        "re.split(\" \",text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVDITB6IV4h",
        "outputId": "1639b368-c6ca-4bea-e141-41d562ca220c"
      },
      "source": [
        "text=\"사과+딸기+수박+메론+바나나\"\n",
        "re.split(\"\\+\",text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XJK1I7rIY4h"
      },
      "source": [
        "##### (3) re.findall()\n",
        "- 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_L2a6yCIYn5",
        "outputId": "2d161a83-5f00-4494-b32e-2276785f74fa"
      },
      "source": [
        "import re\n",
        "text=\"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"  \n",
        "re.findall(\"\\d+\",text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['010', '1234', '1234', '30']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oULAjoPKNIR",
        "outputId": "25610c67-ec54-495a-ea5b-e846211fbfc7"
      },
      "source": [
        "re.findall(\"\\d+\", \"문자열입니다.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxPYlUGIMzXi"
      },
      "source": [
        "##### (4) re.sub()\n",
        "- 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Lzz0QbEiM-ZR",
        "outputId": "9f066dc5-97ae-422c-fb39-eaa7917a03f2"
      },
      "source": [
        "text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "re.sub('[^a-zA-Z]',' ',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2e-SVVmNCRB"
      },
      "source": [
        "#### 5. 정규 표현식 텍스트 전처리 예제\n",
        "- '\\s+'는 공백을 찾아내는 정규표현식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXmPXUbSNNNw",
        "outputId": "073ed501-974e-4d91-db86-8b43348287ca"
      },
      "source": [
        "text = \"\"\"100 John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\"  \n",
        "\n",
        "re.split('\\s+', text) # 공백을 기준으로 분리"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOKOsUeINTZJ"
      },
      "source": [
        "- \\d는 숫자에 해당되는 정규표현식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGy5APotNqeh",
        "outputId": "139ce12a-df0d-4b50-fcad-948b5162ce8b"
      },
      "source": [
        "re.findall('\\d+',text)  # 데이터에서 숫자만 뽑음"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', '101', '102']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfgZtUVwOwXJ",
        "outputId": "c89055bf-4a74-4169-93bf-23fb76a7f8e6"
      },
      "source": [
        "re.findall('[A-Z]',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGQSRMVLO335",
        "outputId": "0f311f1d-62f2-4ea5-d6df-242b6f3d542f"
      },
      "source": [
        "re.findall('[A-Z]{4}',text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROF', 'STUD', 'STUD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR0tQG-sO42B",
        "outputId": "b04d4e87-a44c-426a-aff0-906689570f71"
      },
      "source": [
        "re.findall('[A-Z][a-z]+',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John', 'James', 'Mac']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmkW7jGeccf_"
      },
      "source": [
        "#### 6. 정규 표현식을 이용한 토큰화\n",
        "- NLTK에서는 정규 표현식을 사용해서 단어 토큰화를 수행하는 RegexpTokenizer() 지원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZFkICuAcsfz",
        "outputId": "a1e23292-1d79-4df0-b1ca-7123a2280d51"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(\"[\\w]+\")  # 문자 또는 숫자가 1개 이상인 경우를 인식\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yeddTvXdIBF"
      },
      "source": [
        "- 괄호 안에 토큰을 나누기 위한 기준을 입력 가능\n",
        "  - gaps=true: 정규 표현식을 토큰으로 나누기 위한 기준으로 사용한다는 의미"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqX-llrZdGKn",
        "outputId": "d4c11ca8-2cec-4e89-f45d-359e4b9635a7"
      },
      "source": [
        "tokenizer=RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-sVkGFed4b7"
      },
      "source": [
        "---\n",
        "### 06) 정수 인코딩(Integer Encoding)\n",
        "- 자연어 처리에서 텍스트를 숫자로 바꾸기 위해 첫 단계로 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 수 있다.\n",
        "- 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 전처리 또는 빈도수가 높은 단어들만 사용하기 위해서 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여\n",
        "#### 1. 정수 인코딩(Integer Encoding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KocYSeaZaN8x"
      },
      "source": [
        "##### 1) dictionary 사용하기\n",
        "- 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zezlqMzHgJh3"
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lC9NdMEgrJK"
      },
      "source": [
        "- 문장 토큰화\n",
        "  - 문장 단위로 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiZSu_yTgtPW",
        "outputId": "6de38c37-7f2e-4f6a-e46b-3376217177cf"
      },
      "source": [
        "text = sent_tokenize(text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU5wFyU1g1GW"
      },
      "source": [
        "- 정제 작업을 병행하며, 단어 토큰화를 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7_IOl3Og3fn",
        "outputId": "37ff101b-7ba9-4818-bcfc-b7323bc34774"
      },
      "source": [
        "vocab = {} # 파이썬의 dictionary 자료형\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
        "    result = []\n",
        "\n",
        "    for word in sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    sentences.append(result) \n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3CAm9XlhEoe"
      },
      "source": [
        "- vocab에 중복을 제거한 단어와 각 단어에 대한 빈도수 기록됨\n",
        "  - 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cd-rPuzhKa7",
        "outputId": "fe519485-a73c-4521-b9f6-9f92a1c40f32"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQyqbNfhagp"
      },
      "source": [
        "- 빈도수가 높은 순서대로 정렬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxlbkMMOhbc-",
        "outputId": "39f63dcb-9c9c-44e5-90f7-ddaa669f64b4"
      },
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvU6JsR1hd8-"
      },
      "source": [
        "- 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여, 빈도수 적은 단어 제외"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIGvF57Qhgdv",
        "outputId": "09958710-52b1-40b2-9c59-430f1c21bbf7"
      },
      "source": [
        "word_to_index = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
        "        i=i+1\n",
        "        word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA4vBgH_iI9b",
        "outputId": "62f59a5e-df96-4d42-b152-749edde7aba6"
      },
      "source": [
        "# 상위 n개 단어만 사용\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwo4n2y1pVPu"
      },
      "source": [
        "- sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩, 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어 존재\n",
        "- Out-Of-Vocabulary 'OOV': 단어 집합에 없는 단어\n",
        "  - word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6Xs3JXqlKK"
      },
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0saC8TEWrU3W"
      },
      "source": [
        "- word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNPtDjJurpN4",
        "outputId": "c82b4d42-17dd-4cce-c561-7be66efe3d75"
      },
      "source": [
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8oJbRtctO8_"
      },
      "source": [
        "- 좀 더 쉽게 하기 위해서 Counter, FreqDist, enumerate 또는 케라스 토크나이저를 사용하는 것을 권장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyySaS1vtMCN"
      },
      "source": [
        "##### 2) Counter 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFI5GKmztT1e",
        "outputId": "8b37da2b-e1a1-4656-8fa8-e2913b87f801"
      },
      "source": [
        "from collections import Counter\n",
        "print(sentences)  # 단어 토큰화가 된 결과 저장되어 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJz-bHnataBd"
      },
      "source": [
        "- sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 제작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYkbVDjpwLq9",
        "outputId": "61cc3cff-59e2-4ff5-9291-f712dc195ece"
      },
      "source": [
        "words = sum(sentences, [])\n",
        "# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKIz0TTHw88j"
      },
      "source": [
        "- 파이썬의 Counter()의 입력으로 사용하면 중복을 제거하고 단어의 빈도수를 기록"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTy8ljumw_E_",
        "outputId": "8b905230-a439-4397-ed51-f6b32826d8c1"
      },
      "source": [
        "vocab = Counter(words) # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산 가능\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvBdyDR6xGJ2"
      },
      "source": [
        "- most_common(): 상위 빈도수를 가진 주어진 수의 단어만을 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxgeo_GexJpM",
        "outputId": "49029824-ba3c-45c9-8e04-4180cff70712"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfDDHB1sxJWk"
      },
      "source": [
        "- 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX5iiNq0xRAH",
        "outputId": "f7339987-5b32-47aa-a4c9-f0275eaba35c"
      },
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab :\n",
        "    i = i+1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e9q-NUWxV9s"
      },
      "source": [
        "##### 3) NLTK의 FreqDist 사용하기\n",
        "- NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID1LQnSQxbGk"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlJj2V-1DVk"
      },
      "source": [
        "# np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...\n",
        "vocab = FreqDist(np.hstack(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNIpkHbA1SsJ",
        "outputId": "74484cb3-2ebd-4c6b-a106-d1055c4a365f"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVfVXHFm1Sa9"
      },
      "source": [
        "- 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여(enumerate()를 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6rPBEzT1nyE",
        "outputId": "f853fe76-69b0-4525-ffe5-794ad2cc0554"
      },
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp3VM2sh5Ezi"
      },
      "source": [
        "#### 2. 케라스(Keras)의 텍스트 전처리\n",
        "- 케라스(Keras)는 기본적인 전처리를 위한 도구들 제공\n",
        "  - 케라스의 전처리 도구인 토크나이저\n",
        "  - fit_on_texts: 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여, 정수 인코딩 작업\n",
        "  - word_index: 각 단어에 인덱스가 어떻게 부여되었는지 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-NVOz8_6RML"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HYrOz-F6f_F",
        "outputId": "256a8877-6da5-48cb-8fd3-dac443f033cf"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6CgXz6aG5JE"
      },
      "source": [
        "- word_counts: 각 단어가 카운트를 수행하였을 때 몇 개였는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNSAvG3aHSv4",
        "outputId": "88e1d77f-3eff-4742-a4cd-c950a622c9ac"
      },
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8XmFsLrJv3L"
      },
      "source": [
        "- texts_to_sequences(): corpus의 각 단어를 정해진 인덱스로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkggJr3HKUzz",
        "outputId": "8536b2b5-fdaf-46e4-fa81-136d3447aff8"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWfcGq1cLS_T"
      },
      "source": [
        "- 빈도수 높은 단어 n개\n",
        "  - tokenizer = Tokenizer(num_words=숫자)\n",
        "  - num_words는 0부터 세기 때문에 +1 해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyVhcc2QLXOD"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRVUW01LwE2",
        "outputId": "485e053b-5b24-4ee2-8f31-79f90b01cbae"
      },
      "source": [
        "print(tokenizer.word_index) # 5개만인데 13개 단어 출력"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YR5gNFiL-47",
        "outputId": "a33753f1-6932-41a1-863a-b2915f892078"
      },
      "source": [
        "print(tokenizer.word_counts)  # 마찬가지 13개"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0yio15dMDab"
      },
      "source": [
        "- 실제 적용은 texts_to_sequences()에서"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYiszkRLzrE",
        "outputId": "537389bc-a5f1-489c-e492-058e237f6968"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))    # 5개 단어"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNVm3VgMT-z"
      },
      "source": [
        "- word_index, word_counts에서도 적용하고 싶다면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkkBzznJMbPg",
        "outputId": "017cb7a1-baee-4929-84b6-811f859c0d61"
      },
      "source": [
        "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKY_HKnHMgdi"
      },
      "source": [
        "- OOV: keras tokenizer는 단어를 정수로 바꾸는 과정에서 아예 제거\n",
        "  - 단어 집합에 없는 단어들 보존하고 싶다면 oov_token을 사용\n",
        "  - OOV의 인덱스 == 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G19fnNkMx8u",
        "outputId": "338722f6-a042-4c7f-ade6-e1f8079c5f31"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HHOUWz8M2NS"
      },
      "source": [
        "- 정수 인코딩 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI-qsPWtS8zQ",
        "outputId": "285f097c-5f38-4eaa-c9ea-c53602cc4a37"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJyzLWuMTAJJ"
      },
      "source": [
        "- 빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x7-DWBcTDla"
      },
      "source": [
        "---\n",
        "### 07) 패딩(Padding)\n",
        "- 자연어 처리에서 각 문장(문서)은 서로 길이가 다를 수 있다.\n",
        "- 기계는 길이가 전부 동일한 문서들을 하나의 행렬로 보고, 한꺼번에 묶어서 처리\n",
        "- 즉, 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXAzDXdJabLH"
      },
      "source": [
        "#### 1. Numpy로 패딩하기\n",
        "- 패딩(padding): 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejtpwyyfT1hi"
      },
      "source": [
        "# data\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssbt8YsFT4oB"
      },
      "source": [
        "# 정수 인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL3aeYcJT6Vy",
        "outputId": "6b8cb413-3b91-4405-a7cc-89f28f3084d3"
      },
      "source": [
        "# 텍스트시퀀스의 모든 단어들을 정수로 맵핑\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYo2LwLdULdt"
      },
      "source": [
        "- 최장 문장의 길이 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEEn2Gz_URIZ",
        "outputId": "60f0ed08-fe2f-4069-a6f3-ed240c7d255a"
      },
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbBCT62oUS6B"
      },
      "source": [
        "- 제로 패딩(zero padding)\n",
        "  - 가상의 단어 'PAD'를 사용, 길이가 7보다 짧은 문장에는 숫자 0을 채워서 전부 길이 7로 맞춤\n",
        "  - 기계는 0번 단어 무시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I52Zi1uUUnHJ",
        "outputId": "560c1fd4-f931-4ed6-d8a5-a83d4d349728"
      },
      "source": [
        "for item in encoded: # 각 문장에 대해서\n",
        "    while len(item) < max_len:   # max_len보다 작으면\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szbyzW5-VCOJ"
      },
      "source": [
        "#### 2. 케라스 전처리 도구로 패딩하기\n",
        "- 도구 pad_sequences()를 제공\n",
        "- 기본적으로 문서 앞에 0채움\n",
        "- \"padding='post'\"인자로 줘서 문서 뒤에 0채움, Numpy를 이용한 padding과 동일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JKggatVLMx",
        "outputId": "315a5bfb-b7fd-4686-929b-64e8751c023e"
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXrxBL9EVIOK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jT25cIbVNTB",
        "outputId": "2a8a0474-6528-4c2e-dabd-6e798b2da3d9"
      },
      "source": [
        "padded = pad_sequences(encoded)\n",
        "padded"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
              "       [ 0,  0,  0,  0,  1,  8,  5],\n",
              "       [ 0,  0,  0,  0,  1,  3,  5],\n",
              "       [ 0,  0,  0,  0,  0,  9,  2],\n",
              "       [ 0,  0,  0,  2,  4,  3,  2],\n",
              "       [ 0,  0,  0,  0,  0,  3,  2],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  2],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew5mJkLVVlsF",
        "outputId": "54317633-8fd4-49a2-cbd1-32334d76816c"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post')\n",
        "padded"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7AbQTc4Vzba"
      },
      "source": [
        "- 길이에 제한을 둔 padding\n",
        "  - \"maxlen = n\" 인자 추가: 해당 정수로 모든 문서의 길이를 동일하게 함\n",
        "  - n 보다 긴 문서는 데이터 손실"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeHTcBgFV3vk",
        "outputId": "277477de-38d7-4ed4-bfd1-e9847bcc0746"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)\n",
        "padded"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0],\n",
              "       [ 3,  2,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0],\n",
              "       [ 3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9FkK9GWXAZ"
      },
      "source": [
        "- 0이 아닌 다른 숫자를 PAD로 사용\n",
        "  - \"value = n\" 인자 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it1EKvOhWjOX",
        "outputId": "5c096e57-99ec-412d-95f8-4319a8060fb7"
      },
      "source": [
        "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\n",
        "print(last_value)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uirzF7F2WleJ",
        "outputId": "e5fd90c2-1c13-47ea-f1eb-8fecc4bc6cb5"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', value = last_value)\n",
        "padded"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5, 14, 14, 14, 14, 14],\n",
              "       [ 1,  8,  5, 14, 14, 14, 14],\n",
              "       [ 1,  3,  5, 14, 14, 14, 14],\n",
              "       [ 9,  2, 14, 14, 14, 14, 14],\n",
              "       [ 2,  4,  3,  2, 14, 14, 14],\n",
              "       [ 3,  2, 14, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  2, 14, 14, 14, 14],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlBy1PV-Wui1"
      },
      "source": [
        "---\n",
        "\n",
        "### 08) 원-핫 인코딩(One-Hot Encoding)\n",
        "- 문자를 숫자(벡터)로 바꾸는 기법 중 가장 기본적인 표현 방법\n",
        "- 단어 집합(vocabulary), 사전\n",
        "  - 서로 다른 단어들의 집합\n",
        "  '- book과 books와 같이 단어의 변형 형태도 다른 단어로 간주\n",
        "- 단어집합을 만들고 정수 인코딩하고 벡터로 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQwoAuDKbGIU"
      },
      "source": [
        "#### 1. 원-핫 인코딩(One-Hot Encoding)이란?\n",
        "- 단어 집합의 크기를 벡터의 차원으로, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
        "- 원-핫 벡터(One-Hot vector)\n",
        "- one-hot encoding 과정\n",
        "  - 1) 각 단어에 고유한 인덱스 부여(정수 인코딩)\n",
        "  - 2) 인덱스 위치에 1 또는 0 부여\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqShQc3yhZYm"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wcYHwGtbgJ8",
        "outputId": "60dd1da4-3e61-4240-e588-b3532cfd2cac"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(token)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bymJ12v_hj9b",
        "outputId": "21b4de49-0757-4f20-da7f-947874028f47"
      },
      "source": [
        "word2index={}\n",
        "for voca in token:\n",
        "     if voca not in word2index.keys():\n",
        "       word2index[voca]=len(word2index)\n",
        "print(word2index)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhDPTtwohoeQ"
      },
      "source": [
        "def one_hot_encoding(word, word2index):\n",
        "       one_hot_vector = [0]*(len(word2index))\n",
        "       index=word2index[word]\n",
        "       one_hot_vector[index]=1\n",
        "       return one_hot_vector"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz0zz8DthplY",
        "outputId": "40e4a8a2-f002-4caf-ec9b-d8c8628924af"
      },
      "source": [
        "one_hot_encoding(\"자연어\",word2index)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOfmerG8fVn-"
      },
      "source": [
        "#### 2. 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)\n",
        "- to_categorical()를 지원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCIHeJrh6Br"
      },
      "source": [
        "- 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-oYliIqhs1Z",
        "outputId": "4fad1bb9-126b-4dd1-9f9c-ce3d6bb085e7"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qy-wMI6iCiO",
        "outputId": "4ffcbd48-2a80-4951-cefb-e43d3507e60b"
      },
      "source": [
        "# sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"    # 일부 단어\n",
        "encoded=t.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylyWqA4EhoKA"
      },
      "source": [
        "- one-hot encoding:  to_categorical()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz4cZOW7iYbU",
        "outputId": "a94bdf73-685b-4547-833f-ea5874850017"
      },
      "source": [
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yy1lp5QicBV"
      },
      "source": [
        "#### 3. 원-핫 인코딩(One-Hot Encoding)의 한계\n",
        "- 1) 단어의 개수가 늘어날수록, 필요한 벡터 저장공간 계속 늘어남\n",
        "- 2) 단어의 유사도를 표현하지 못함\n",
        "  - 이는 검색 시스템 등에서 심각한 문제\n",
        "- 해결방법\n",
        "  - 1) 카운트 기반 벡터화: LSA, HAL 등\n",
        "  - 2) 예측 기반 벡터화: NNLM, RNNLM, Word2Vec, FastText 등 \n",
        "  - 3) 카운트 + 예측 기반: GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBPy-RGpjUuq"
      },
      "source": [
        "---\n",
        "### 09) 데이터의 분리(Splitting Data)\n",
        "- 데이터 훈련을 위해 데이터를 적절히 분리하는 작업 필요\n",
        "- 지도학습(supervised learning)을 위한 데이터 분리 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244RS6p2jocw"
      },
      "source": [
        "#### 1. 지도 학습(Supervised Learning)\n",
        "- 훈련데이터\n",
        "  - x_train: 문제지 데이터\n",
        "  - y_train: 문제지에 대한 정답\n",
        "- 테스트데이터\n",
        "  - x_test: 시험지 데이터\n",
        "  - y_test: 시험지에 대한 정답 데이터\n",
        "-  X_train과 y_train에 대해서 학습하고, 기계에게 y_test는 보여주지 않고, X_test에 대해서 정답을 예측하게 함. 기계가 예측한 답과 실제 정답인 y_test를 비교하면서 기계가 정답을 얼마나 맞췄는지 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG50Sd0dk1qx"
      },
      "source": [
        "#### 2. X와 y분리하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO-JVBrWk30z"
      },
      "source": [
        "##### 1) zip 함수를 이용하여 분리하기\n",
        "- zip(): 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxDOSJoNlDDH",
        "outputId": "b8bd5616-27d8-4181-c442-c0f5155a6233"
      },
      "source": [
        "sequences=[['a', 1], ['b', 2], ['c', 3]] # 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.\n",
        "X,y = zip(*sequences) # *를 추가\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a', 'b', 'c')\n",
            "(1, 2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkVcidvAm3c8"
      },
      "source": [
        "##### 데이터프레임을 이용하여 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "GI9mboLGm6RS",
        "outputId": "1437d879-469c-4ca7-db18-1c9e7ad6d12e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
        "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
        "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
        "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
        "columns = ['메일 본문', '스팸 메일 유무']\n",
        "\n",
        "df = pd.DataFrame(values, columns=columns)\n",
        "df"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>메일 본문</th>\n",
              "      <th>스팸 메일 유무</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>당신에게 드리는 마지막 혜택!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    메일 본문  스팸 메일 유무\n",
              "0        당신에게 드리는 마지막 혜택!         1\n",
              "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
              "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
              "3  (광고) AI로 주가를 예측할 수 있다!         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j-uqRjEnBjT",
        "outputId": "75eef875-e706-482e-f987-2486b424e7c4"
      },
      "source": [
        "X=df['메일 본문']\n",
        "y=df['스팸 메일 유무']\n",
        "print(X, '\\n')\n",
        "print(y)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0          당신에게 드리는 마지막 혜택!\n",
            "1      내일 뵐 수 있을지 확인 부탁드...\n",
            "2      도연씨. 잘 지내시죠? 오랜만입...\n",
            "3    (광고) AI로 주가를 예측할 수 있다!\n",
            "Name: 메일 본문, dtype: object \n",
            "\n",
            "0    1\n",
            "1    0\n",
            "2    0\n",
            "3    1\n",
            "Name: 스팸 메일 유무, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnEATuxknZ5p"
      },
      "source": [
        "##### 3) Numpy를 이용하여 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEZFAVsKncJM",
        "outputId": "80d166bf-3567-4e90-a935-6e6546ecbb88"
      },
      "source": [
        "ar = np.arange(0,16).reshape((4,4))\n",
        "print(ar)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnYsWoHxndzY",
        "outputId": "0231467f-f6fd-46dd-fb17-6aff3d767151"
      },
      "source": [
        "X=ar[:, :3]\n",
        "print(X)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 4  5  6]\n",
            " [ 8  9 10]\n",
            " [12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2x1O_DVnezt",
        "outputId": "acd790f0-fb3a-4485-86dc-b8d37bda1423"
      },
      "source": [
        "y=ar[:,3]\n",
        "print(y)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3  7 11 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpmFSUv5nhAa"
      },
      "source": [
        "#### 3. 테스트 데이터 분리하기\n",
        "- X와 y가 분리된 데이터에 대해서 테스트 데이터를 분리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5mmbrf9nm_Q"
      },
      "source": [
        "##### 사이킷 런을 이용하여 분리하기\n",
        "- 사이킷 런은 학습용 테스트와 테스트용 데이터를 분리하게 해주는 train_test_split를 지원\n",
        "  - X : 독립 변수 데이터(배열이나 데이터프레임)\n",
        "  - y : 종속 변수. 레이블 데이터\n",
        "  - test_size : 테스트용 데이터 개수, 1보다 작은 실수=비율.\n",
        "  - train_size : 학습용 데이터의 개수\n",
        "(test_size와 train_size 중 하나만 기재해도 가능)\n",
        "  - random_state : 난수 시드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ir7G3tenshk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDFsigK7oLV9",
        "outputId": "8534060d-3951-409e-f3b5-2680ac15bbea"
      },
      "source": [
        "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
        "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
        "print(X)\n",
        "print(list(y)) #레이블 데이터"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [2 3]\n",
            " [4 5]\n",
            " [6 7]\n",
            " [8 9]]\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEcd2aBhoV7X"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
        "#3분의 1만 test 데이터로 지정.\n",
        "#random_state 지정으로 인해 순서가 섞인 채로 훈련 데이터와 테스트 데이터가 나눠진다."
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdEYqqxboV2n",
        "outputId": "2a6f4583-a9d3-4ae2-fb8c-f4265015c511"
      },
      "source": [
        "print(X_train)\n",
        "print(X_test)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3]\n",
            " [4 5]\n",
            " [6 7]]\n",
            "[[8 9]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtkcjLkMok7-",
        "outputId": "4b4db3f7-abde-4344-f3e6-af359cff8a55"
      },
      "source": [
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n",
            "[4, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QcjDbnwnsKA"
      },
      "source": [
        "##### 2) 수동으로 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyS_StrZooKv",
        "outputId": "5bb8abef-5bcc-46f0-b0f1-75e384b19418"
      },
      "source": [
        "X, y = np.arange(0,24).reshape((12,2)), range(12)\n",
        "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
        "print(X, '\\n')\n",
        "print(list(y))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1]\n",
            " [ 2  3]\n",
            " [ 4  5]\n",
            " [ 6  7]\n",
            " [ 8  9]\n",
            " [10 11]\n",
            " [12 13]\n",
            " [14 15]\n",
            " [16 17]\n",
            " [18 19]\n",
            " [20 21]\n",
            " [22 23]] \n",
            "\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZzXk_PBoykZ"
      },
      "source": [
        "- n_of_train: 훈련 데이터의 개수\n",
        "- n_of_test: 테스트 데이터의 개수\n",
        "  - n_of_train을 len(X) * 0.8로 구했듯이 n_of_test 또한 len(X) * 0.2로 계산하면 데이터에 누락 발생\n",
        "  - 어느 한 쪽을 먼저 계산하고 그 값만큼 제외하는 방식으로 계산해야 함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX9nZUYCo3gn",
        "outputId": "8049dd84-f6ee-4302-944b-052323f6262c"
      },
      "source": [
        "n_of_train = int(len(X) * 0.8) # 데이터의 전체 길이의 80%에 해당하는 길이값을 구한다.\n",
        "n_of_test = int(len(X) - n_of_train) # 전체 길이에서 80%에 해당하는 길이를 뺀다.\n",
        "print(n_of_train)\n",
        "print(n_of_test)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANHljcEpQF9"
      },
      "source": [
        "X_test = X[n_of_train:] #전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
        "y_test = y[n_of_train:] #전체 데이터 중에서 20%만큼 뒤의 데이터 저장\n",
        "X_train = X[:n_of_train] #전체 데이터 중에서 80%만큼 앞의 데이터 저장\n",
        "y_train = y[:n_of_train] #전체 데이터 중에서 80%만큼 앞의 데이터 저장"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlYW9931pV0Y",
        "outputId": "f1bb1841-5d04-47dc-f898-76b8f90e4290"
      },
      "source": [
        "print(X_test)\n",
        "print(list(y_test))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[18 19]\n",
            " [20 21]\n",
            " [22 23]]\n",
            "[9, 10, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSPSQFypbbZ"
      },
      "source": [
        "--- \n",
        "### 10) 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)"
      ]
    }
  ]
}