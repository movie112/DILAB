{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Paper Review] encoder-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2AGZaAK/Sk0ei04n1X1XJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/210908_210914/attention_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKsV8kNpOoV8"
      },
      "source": [
        "# [Paper Review] encoder-decoder: Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "## Bahdanau Attention\n",
        "- 2015\n",
        "- [논문](https://arxiv.org/abs/1409.0473)\n",
        "- [참고](https://www.youtube.com/watch?v=S2msiG9g7Us&list=PL7uFRNKOioUq3ghhvhXahukKx62N3OpmV&index=4)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNuCIL-Qu-T"
      },
      "source": [
        "## Backgraound\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW7VdACoZien"
      },
      "source": [
        "### 규칙 기반 기계 번역(rule-based machine translation, RBMT)\n",
        "- 형태소 분석(morphological analysis), 구문 분석(syntactic), 의미 합성(semantic composition)의 과정을 거처 중간 언어를 만든 다음 역의 과정을 거쳐 중간 언어를 타깃 언어의 단어로 맵핑\n",
        "- 완벽한 기계번역 해낼 수 없다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0R_skV-cIAv"
      },
      "source": [
        "### 통계적 기계 번역(statistical machine translation, SMT)\n",
        "- Neural Network 와 비슷\n",
        "- 단어 대 단어로 매칭하여 출현 확률이 몿은 것으로 모델 생성: (나는) 박물관에 갔다. -> I went to the museum. 무주어 현상 해결x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptUbFpmwYCtB"
      },
      "source": [
        "### Neural Machine Translation (NMT)\n",
        "- phrase-based machine translation 에서 발전\n",
        "- 대표적 구조: encoder-decoder 구조\n",
        "- 장점: 번역하고자 하는 데이터 쌍(pair)만을 필요로 함: End to End frame work\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2017/10/Depiction-of-Sutskever-Encoder-Decoder-Model-for-Text-Translation.png\" width=\"600px\" height=\"200px\"></img>\n",
        "    - a, b, c -> w, x, y, z\n",
        "\n",
        "- 입력 문장을 문장 벡터 형태로 변환, 이를 기반으로 출력 언어의 문장을 생성하는 방식으로 번역\n",
        "- 입력 문장을 문장 벡터로 변환하는 인코더와 출력 문장을 생성해내는 디코더는 인공신경망으로 구성\n",
        "- 데이터를 통해 인코더, 디코더의 parameter 값이 조정됨.\n",
        "\n",
        "- 목표: maximize the probability of a correct translatio given a source sentence.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqWlOpF9YWS3"
      },
      "source": [
        "#### encoder-decoder 구조의 문제점\n",
        "- 고정된 크기의 벡터c가 갖는 bottleneck 문제\n",
        "  - encoder의 정보를 넘겨주는 수단으로 __고정된 크기의 벡터__를 사용\n",
        " 있다 ## encoder-decoder 구조의 문제점\n",
        "- 고정된 크기의 벡터c가 갖는 bottleneck 문제\n",
        "  - encoder의 정보를 넘겨주는 수단으로 __고정된 크기의 벡터__를 사용\n",
        "  - encoder의 내용을 압축해서 전달하므로 길이가 긴 source sentence에 대한 충분한 정보를 담을 수 없음(정보 유실)\n",
        "  - 충분한 성능 향상에 대한 방해 요소\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1000/1*1ui7iDq956eDs-mAZHEdIg.png\" width=\"550px\" height=\"200px\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n7vFk5TefpK"
      },
      "source": [
        "#### seq2seq\n",
        "- neural network machine translation은 모두 seq2seq 구조 가짐\n",
        "- source 와 target이 존재하고, 중간에 매개가 되는 모델이 존재\n",
        "  - 그런 모델을 바탕으로 한 neural network 구조\n",
        "\n",
        "- end to end: SMT는 여러 모듈로 구성, 시스템 복잡도가 높음, NMT는 단 한 개의 언어모델 생성\n",
        "- better performance: SMT의 n-gram 희소 문제 해결\n",
        "- better embedding: better hidden-state vector, 문장 사이사이에 발생할 노이즈나 희소성 문제 대처"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EY0RWxlYJh9"
      },
      "source": [
        " ### [제안]\n",
        "- 모델이 소스 시퀀스의 일부를 자동으로 탐색할 수 있는 방법을 제안: handle this issue by allowing a model to automatically search for parts of a source\n",
        "- -> 입력 시퀀스를 매 step마다 vector representaion해서 벡터 시퀀스로 인코딩한 후, 매 디코딩 step마다 벡터 시퀀스의 subset을 adaptive하게 선택(어느 step을 참고할 것인지 adaptive하게 선택하는 방법 제안)\n",
        "---\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCskMCeNaNah"
      },
      "source": [
        "## Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf-bRIe8aQ8q"
      },
      "source": [
        "### Bahdanau attention process\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBWTIdnIDwG"
      },
      "source": [
        "#### 1. [encoder hidden states] : source sequence가 RNN 기반 encoder에 들어간다. (ex. RNN, Gru, LSTM)\n",
        "  - forward 방향 진행할 수도 있고, backward 방향도 가능 -> __bi-directional__ 모델 제시\n",
        "    - 즉, hidden state가 2단으로 이루어져 있음 -> bi-directonal RNN(RNN, Gru, LSTM)임\n",
        "    - 각 단어에 대해 forward, backward를 해서 앞, 뒤 단어의 정보를 가질 가능성 높다.: hidden state가 앞뒤 단어 정보 저장\n",
        "  - 두 과정을 거치고 나면, 각 source token에 대한 vector representaion이 결과로\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePfaYmBkIIMi"
      },
      "source": [
        "#### 2. [decoder hidden states]: t번 째 step의 hidden state를 구하는 과정에서 attention 적용\n",
        "  - t-1번 째 hidden state -> FC layer \n",
        "  - encoder의 각 representation 마다 FC layer 통과\n",
        "  - 둘을 더해서 tanh 적용하고 FC layer 또 통과시킨 후 input sequence과 동일한 길이의 score을 산출?\n",
        "  - softmax 거쳐서 하나의 분포로 만듦: attention distribution\n",
        "  - attention distribution은 가중치 역할으로 입력 시퀀스와 같은 길이, 둘을 곱하고 weighted sum -> 최종 context vector 완성\n",
        "  - (context vector와 t-1의 output)는 t 시점의 입력으로 사용 -> t의 hidden state 구함\n",
        "  - decoder hidden state -> FC layer -> softmax -> __t 시점의 output 예측__\n",
        "  <img src=\"https://lh3.googleusercontent.com/proxy/azPzLI2wHB_lYa3QQx-Dxly4DQsphSPlAQLbwz6zG6yvqBeyZ9gpeNx2jddkVicwq3R9oFgvqWazwS9G9hTcZV-VzntOYS5reidik2qvngcATpRfP1AlLjM\" width=\"600px\" height=\"500px\"></img> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2P75hZQlMpy"
      },
      "source": [
        "---\n",
        "## Abbreviations\n",
        "- f: LSTM function\n",
        "- q: forward RNN\n",
        "- alpha: feedforward NN\n",
        "- c: __sequence of hidden states(context vector)__\n",
        "- f: encoder's time-step\n",
        "- i: decoder's time-step\n",
        "- Align: attention\n",
        "- Annotation: hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYm-m7fFDw0e"
      },
      "source": [
        "---\n",
        "## Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9wUqav6HsBg"
      },
      "source": [
        "### Quantative results_1\n",
        "- baseline 보다 전반적으로 좋은 성능\n",
        "- 고정된 크기의 벡터에 긴 길이의 정보를 담는 문제가 있다고 했고, 긴 길이의 sequence가 더 많은 정보를 가질 것이라고 기대했으나 짧은 sequence에 대해 attention을 적용한 결과가 더 성능이 좋았다.??????????????????\n",
        "  - dataset: VMT'14(Englisj -> French)\n",
        "  - NO UNK: sentences without any unknown word\n",
        "  - RNNencdec: baseline(attention 적용x)\n",
        "  - search: Bahdanau attention 적용\n",
        "  - RNNserch-50*: 다른 실험보다 오래 더이상 성능향상이 없는 시간까지 학습한 결과(Trained much longer until the performance on the dev set stopped improving)\n",
        "  - Moses: conventiona phrase-based translaton system(using seperate monolingual corpus)\n",
        "  - 숫자는 sequence의 length 의미\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC57gXuSHl9c"
      },
      "source": [
        "---\n",
        "## Additional\n",
        "- 보통 attention 크게 두 가지 떠올림\n",
        "<img src=\"https://blog.kakaocdn.net/dn/cUN1zY/btq6pVl1Mzl/KTKb1uMH2YtkajglwbGB7k/img.png\" width=\"600px\" height=\"390px\"></img>\n",
        "- attention 방법과 적용 시점에서 차이 존재\n",
        "  - Luong: t시점의 state를 구한 것을 가지고 context vector를 추출하고 t시점의 hidden state와 결합하여 prediction\n",
        "  - 따라서, Bahdanau: t-1시점 / Luong: t시점 으로 attention을 구함\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy0z9BuCOnFN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}