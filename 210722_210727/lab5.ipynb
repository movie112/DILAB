{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1UieOvaewryIr6bjjfJrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CDKcjC8NVYr"
      },
      "source": [
        "# Lab5: LAS(Listen Attend Spell)\n",
        "## contents\n",
        "- CTC의 dynamic programming   \n",
        "  - <https://distill.pub/2017/ctc/>\n",
        "- LAS\n",
        "  - CTC vs LAS\n",
        "---\n",
        "\n",
        "## CTC의 dynamic programming\n",
        "- 단순히 학습이 끝난 지점에서 x spectogram이 주어졌을 때 나오는 path의 확률값이 가장 큰 것을 argmax 취하는 것이 그다지 적절하지 않다.\n",
        "- 많은 relevant한 path 중에서 좋은 path를 찾는 방법을 모두 다 연산하자니 가능한 path가 너무 많아짐\n",
        "\n",
        "<img src=\"https://distill.pub/2017/ctc/assets/ctc_cost.svg\" width=\"400px\" height=\"250px\"></img>\n",
        "\n",
        "- output이 a, b 뿐이라고 가정, output sequence 사이에 입실론 존재한다고 가정(구분인자, 입실론으로 갈 수도 안 안갈 수도 있음)\n",
        "  - 시작이 입실론(묵음) 아니면 a -> 입실론 다음엔 입실론 아니면 a -> 반복 -> 아무리 늦어도 4번에는 a가 나와야 함(대부분의 사람은 sequence2일 때 a를 발음)\n",
        "  - a 다음에는 입실론 or a or b , 입실론 다음 a는 안됨\n",
        "- output length에 맞춰 단어들이 등장해야 하고 단어에 의해서 어느정도 순서 강제 됨 등등 (규칙)  -> 모든 path가 존재한다해도 가능한 path 는 저 정도\n",
        "---\n",
        "- case1\n",
        "\n",
        "<img src=\"https://distill.pub/2017/ctc/assets/cost_no_skip.svg\" width=\"200px\" height=\"250px\"></img>\n",
        "\n",
        "- case2\n",
        "\n",
        "<img src=\"https://distill.pub/2017/ctc/assets/cost_regular.svg\" width=\"200px\" height=\"250px\"></img>\n",
        "\n",
        "  - 2개의 case로 recursive하게 form을 설명할 수 있다.\n",
        "  - 가장 relevant한 path는 마지막 알파 값 두 개로 설명할 수 있다.\n",
        "\n",
        "---\n",
        "## LAS\n",
        "#### - CTC vs LAS\n",
        "- CTC   \n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRKSpd2vMvctVF80LFFXlqZFhG53Kzfehuj8w&usqp=CAU\" width=\"250px\" height=\"250px\"></img>\n",
        "  - CTC모델의 기본은 RNN  rkqjsdnk Sequence 모델을 여러층 쌓고, 인코더로 들어오는 입력 데이터는 최종적으로 softmax를 통과해서 출력\n",
        "- LAS   \n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQm93TPGZfXSsNW_VnlV8uyjnmREIm9e8fzVGwm3MNYkxMBDWOK7PP9TNJEyYptkUuLErA&usqp=CAU\" width=\"250px\" height=\"300px\"></img>\n",
        "  - input x와 이전 sequence를  auto-regressive 방식으로 given 받고, 그 다음 y 예측\n",
        "  - attention을 써보자\n",
        "  - auto-regressive: decoding 하는 step에서 들어가는 인자=(given한 input sequence와  이전 step에서 decoding한 정보)로 다음 sequence 예측\n",
        "  \n",
        "#### - LAS 구조\n",
        "<img src=\"https://user-images.githubusercontent.com/7529838/33699263-69206498-db55-11e7-8295-029e0b012f32.png\" width=\"300px\" height=\"350px\"></img>\n",
        "##### 1. Listener\n",
        "- Listener는 Encoder로서 음성 신호를 high level feature들로 변환하는 역할을 LAS내에서 맡고 있다. Listener는 BLSTM을 Pyramidal 형식으로 3개를 붙여서 사용하고 있다. 논문에서는 이를 pBLSTM으로 부르고 있으며, pyramidal 하게 사용하는 이유는 pBLSTM 1개당 연산속도를 2배로 줄여주기 때문이다. \n",
        "\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/9909123C5C9041D83F\" width=\"300px\" height=\"100px\"></img>\n",
        "- h: listen encoder, i: i-th time step, j: j-th layer\n",
        "\n",
        "##### 2, Speller\n",
        "- Speller는 Decoder로서, attention-based LSTM 변환기 역할을 맡고 있다. 즉, 모든 출력 단계에서 변환기는 이전에 본 모든 문자를 조건으로 한 다음 문자에 대한 확률 분포를 생성한다. (attention layer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
