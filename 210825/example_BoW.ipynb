{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_0825.ipynb",
      "provenance": [],
      "mount_file_id": "1BTgnKJrX4R3WdCscau7R4Uf891wG_QY7",
      "authorship_tag": "ABX9TyP0qHYwPWQv9v1k/vMIyZLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/210811_210824/example_0825.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ifp0bAIxGUy"
      },
      "source": [
        "### BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqdRrrdf4PnX",
        "outputId": "8968631a-16d0-4ec8-f3f2-7d70c44cf253"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize # 단어 토큰화\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ3cwVCrxzh6",
        "outputId": "8bd53f33-69a8-428a-d078-bfb8e3a170a4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus1 = ['John likes to watch movies. Mary likes movies too.']\n",
        "\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus1).toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 1 2 1 1 1]]\n",
            "{'john': 0, 'likes': 1, 'to': 4, 'watch': 6, 'movies': 3, 'mary': 2, 'too': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66FuitH2ykh",
        "outputId": "a6a0176c-6311-4da8-a685-8eeae0d4186f"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/movie-review/labeledTrainData.tsv', \n",
        "                    header=0, delimiter='\\t', quoting=3)\n",
        "corpus = df['review'][0][:606]\n",
        "\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hijlJGPyT6B",
        "outputId": "8f4d0d32-31dd-4faa-d764-8387c618888b"
      },
      "source": [
        "def preprocessing(x):\n",
        "  # HTML 태그 제거\n",
        "  x=BeautifulSoup(x, 'html.parser').get_text()\n",
        "  # 특수문자 제거\n",
        "  x=re.sub(\"\\W\", \" \", x)\n",
        "  return x\n",
        "\n",
        "def tokenizing(words):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = word_tokenize(words.lower())\n",
        "  words = [x for x in words if x not in stop_words]\n",
        "  return words\n",
        "\n",
        "p = preprocessing(corpus)\n",
        "t = tokenizing(p)\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(t)\n",
        "vector = token.texts_to_sequences(t)  # vector화\n",
        "\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7], [1], [8], [2], [9], [10], [11], [12], [13], [14], [3], [15], [3], [4], [5], [16], [17], [18], [19], [20], [21], [22], [23], [24], [5], [25], [26], [27], [28], [29], [4], [6], [30], [6], [31], [32], [33], [1], [34], [35], [36], [37], [38], [39], [2], [40], [41], [42], [43], [44], [45], [46], [47], [48]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "325JgByF4qRo",
        "outputId": "f999c195-4e8b-409d-b6d8-2d6eba01af82"
      },
      "source": [
        "print(p)  # preprocessint(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyHOYG0b4JRP",
        "outputId": "ea85be4a-0cdb-46f9-b3a4-5234978b19fd"
      },
      "source": [
        "print(t)  # 토큰화"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epUdOz9-4lPR",
        "outputId": "9831ff3f-4135-446e-daa5-9b6993f75958"
      },
      "source": [
        "print(token.word_index) # 각 단어의 인덱스 값"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'going': 1, 'mj': 2, 'watched': 3, 'moonwalker': 4, 'maybe': 5, 'part': 6, 'stuff': 7, 'moment': 8, 'started': 9, 'listening': 10, 'music': 11, 'watching': 12, 'odd': 13, 'documentary': 14, 'wiz': 15, 'want': 16, 'get': 17, 'certain': 18, 'insight': 19, 'guy': 20, 'thought': 21, 'really': 22, 'cool': 23, 'eighties': 24, 'make': 25, 'mind': 26, 'whether': 27, 'guilty': 28, 'innocent': 29, 'biography': 30, 'feature': 31, 'film': 32, 'remember': 33, 'see': 34, 'cinema': 35, 'originally': 36, 'released': 37, 'subtle': 38, 'messages': 39, 'feeling': 40, 'towards': 41, 'press': 42, 'also': 43, 'obvious': 44, 'message': 45, 'drugs': 46, 'bad': 47, 'kay': 48}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRn7z5si4m-4",
        "outputId": "9e8a4d9c-4dc8-40a7-f11b-e0aac9788415"
      },
      "source": [
        "print(token.word_counts.items())  # 각 단어의 빈도수"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_items([('stuff', 1), ('going', 2), ('moment', 1), ('mj', 2), ('started', 1), ('listening', 1), ('music', 1), ('watching', 1), ('odd', 1), ('documentary', 1), ('watched', 2), ('wiz', 1), ('moonwalker', 2), ('maybe', 2), ('want', 1), ('get', 1), ('certain', 1), ('insight', 1), ('guy', 1), ('thought', 1), ('really', 1), ('cool', 1), ('eighties', 1), ('make', 1), ('mind', 1), ('whether', 1), ('guilty', 1), ('innocent', 1), ('part', 2), ('biography', 1), ('feature', 1), ('film', 1), ('remember', 1), ('see', 1), ('cinema', 1), ('originally', 1), ('released', 1), ('subtle', 1), ('messages', 1), ('feeling', 1), ('towards', 1), ('press', 1), ('also', 1), ('obvious', 1), ('message', 1), ('drugs', 1), ('bad', 1), ('kay', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEXauYnlDRTN"
      },
      "source": [
        "### DTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Ay6ChL_xC3",
        "outputId": "1e97c3a3-e1f9-44f0-fb9e-f01091a5c55a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus_list=[]\n",
        "corpus_list.append(corpus)\n",
        "corpus_list\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "count = vectorizer.fit_transform(corpus_list)\n",
        "count.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1,\n",
              "        2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 2, 1,\n",
              "        4, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiQqD0QlCMKW",
        "outputId": "1ad272d3-6e97-4f55-da3f-f411b7d88513"
      },
      "source": [
        "count.toarray().shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 80)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAn4is7LAy1-",
        "outputId": "140df662-d8ca-4224-f44d-64e0572b4f9e"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'with': 78, 'all': 2, 'this': 64, 'stuff': 60, 'going': 21, 'down': 14, 'at': 6, 'the': 62, 'moment': 43, 'mj': 42, 've': 69, 'started': 59, 'listening': 36, 'to': 66, 'his': 27, 'music': 45, 'watching': 73, 'odd': 48, 'documentary': 13, 'here': 26, 'and': 4, 'there': 63, 'watched': 72, 'wiz': 79, 'moonwalker': 44, 'again': 1, 'maybe': 38, 'just': 34, 'want': 70, 'get': 20, 'certain': 10, 'insight': 30, 'into': 31, 'guy': 23, 'who': 77, 'thought': 65, 'was': 71, 'really': 54, 'cool': 12, 'in': 28, 'eighties': 16, 'make': 37, 'up': 68, 'my': 46, 'mind': 41, 'whether': 75, 'he': 25, 'is': 32, 'guilty': 22, 'or': 50, 'innocent': 29, 'part': 52, 'biography': 8, 'feature': 17, 'film': 19, 'which': 76, 'remember': 56, 'see': 57, 'cinema': 11, 'when': 74, 'it': 33, 'originally': 51, 'released': 55, 'some': 58, 'of': 49, 'has': 24, 'subtle': 61, 'messages': 40, 'about': 0, 'feeling': 18, 'towards': 67, 'press': 53, 'also': 3, 'obvious': 47, 'message': 39, 'drugs': 15, 'are': 5, 'bad': 7, 'kay': 35, 'br': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC4rQwlzA4-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae11efe6-f212-47a6-8e86-daea7e08dbe5"
      },
      "source": [
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 78)\t2\n",
            "  (0, 2)\t1\n",
            "  (0, 64)\t2\n",
            "  (0, 60)\t1\n",
            "  (0, 21)\t2\n",
            "  (0, 14)\t1\n",
            "  (0, 6)\t2\n",
            "  (0, 62)\t7\n",
            "  (0, 43)\t1\n",
            "  (0, 42)\t2\n",
            "  (0, 69)\t1\n",
            "  (0, 59)\t1\n",
            "  (0, 36)\t1\n",
            "  (0, 66)\t4\n",
            "  (0, 27)\t1\n",
            "  (0, 45)\t1\n",
            "  (0, 73)\t1\n",
            "  (0, 48)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 4)\t3\n",
            "  (0, 63)\t1\n",
            "  (0, 72)\t2\n",
            "  (0, 79)\t1\n",
            "  (0, 44)\t2\n",
            "  :\t:\n",
            "  (0, 76)\t1\n",
            "  (0, 56)\t1\n",
            "  (0, 57)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 74)\t1\n",
            "  (0, 33)\t2\n",
            "  (0, 51)\t1\n",
            "  (0, 55)\t1\n",
            "  (0, 58)\t1\n",
            "  (0, 49)\t2\n",
            "  (0, 24)\t1\n",
            "  (0, 61)\t1\n",
            "  (0, 40)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 67)\t1\n",
            "  (0, 53)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 47)\t1\n",
            "  (0, 39)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 9)\t2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0yhauNDVlF"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k81qow5Dcl9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=0.0, analyzer= \"word\", sublinear_tf=True, \n",
        "                             ngram_range=(1,3), max_features=1000, stop_words='english')\n",
        "# min_df: 설정값보다 특정 토큰의 df(document Frequency)가 적으면 벡터화에서 제거\n",
        "# analyzer: word/char 2가지, word는 단위: 단어, char는 단위: char\n",
        "# sublinear_tf: term frequency에 대한 smoothing 여부\n",
        "# ngram_range: n-gram의 범위, 분석기에 의해 설정값을 사용하여 ngram자동 생성\n",
        "# max_features: 벡터의 최대 길이"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix8vorbIEzvF",
        "outputId": "6d64b791-7686-4389-93c1-d6a53b65b414"
      },
      "source": [
        "corpus_list=[]\n",
        "corpus_list.append(corpus)\n",
        "corpus_list\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(corpus_list)\n",
        "tfidf.toarray()# 각 단어의 빈도수"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.21428571,\n",
              "        0.07142857, 0.14285714, 0.07142857, 0.07142857, 0.14285714,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.14285714, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.14285714, 0.14285714, 0.14285714,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.14285714, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.14285714, 0.07142857, 0.14285714,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.14285714,\n",
              "        0.07142857, 0.07142857, 0.14285714, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.5       , 0.07142857, 0.14285714,\n",
              "        0.07142857, 0.28571429, 0.07142857, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.14285714, 0.14285714, 0.07142857, 0.07142857,\n",
              "        0.07142857, 0.07142857, 0.07142857, 0.14285714, 0.07142857]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMr9QYhYFvJF"
      },
      "source": [
        "### word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtG81aafFybs"
      },
      "source": [
        "import logging\n",
        "from gensim.models import word2vec\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# word2vec 입력은 단어로 표현된 리스트\n",
        "# n-gram으로 넣을 수 있지만 여기에선 단순히 split만해서 넣는 것으로 함\n",
        "sentences = []\n",
        "for review in list(df['review']) :\n",
        "    sentences.append(review.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR3yhA3KgB2V"
      },
      "source": [
        "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.   \n",
        "window = 컨텍스트 윈도우 크기   \n",
        "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)   \n",
        "workers = 학습을 위한 프로세스 수   \n",
        "sg = 0은 CBOW, 1은 Skip-gram.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn5RxfwrZMi6"
      },
      "source": [
        "# 하이퍼파라미터\n",
        "num_features =1000  # word2vec 특징 수\n",
        "min_word_count = 20\n",
        "num_workers =6\n",
        "context =10 # word2vec 수행을 위한 context window 크기\n",
        "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-13%EC%9D%BC%EC%B0%A8-word2vec-3c82ec870426\n",
        "downsampling = 1e-3 #Word2vec 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링, 보통 0.001이 좋은 성능\n",
        "#Downsampling of frequent words # 자주 나오는 단어에 대해서는 0.001 만큼 다운 샘플링하여 시간을 아낌"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R77LxhL5bO3G"
      },
      "source": [
        "print(\"Training\")\n",
        "# https://wikidocs.net/50739\n",
        "model = word2vec.Word2Vec(sentences,\n",
        "                         workers = num_workers,\n",
        "                          size = num_features,\n",
        "                          min_count = min_word_count,\n",
        "                          window =  context,\n",
        "                          sample = downsampling,\n",
        "                          iter = 10,\n",
        "                          sg =0 # sg =0 CBOW, 1 : skip-gram\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
