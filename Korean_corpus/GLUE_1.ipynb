{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLUE_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM939BXz+uwiXA7N3Et67iA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/Korean_corpus/GLUE_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV1rLC45PKQ6"
      },
      "source": [
        "# GLUE : General Language Understanding Evaluation\n",
        "<https://huffon.github.io/2019/11/16/glue/>\n",
        "---\n",
        "- 목적: __강건하고 범용적인 자연어 이해 시스템의 개발__\n",
        "- 구성\n",
        "  -자연어 처리 모델을 훈련시키고, 그 성능을 평가 및 비교 분석하기 위한 데이터셋들\n",
        "  - 다양하고 해결하기 어려운 9개의 task dataset으로 구성\n",
        "- 모델들의 자연어 이해 능력을 평가하기 위해 고안, 이제는 BERT와 같은 __전이학습__ 모델들을 평가하기 위한 필수적인 벤치마크\n",
        "- NLU(Natural Language Understanding) 자연어 이해 : 자연어 형태의 문장을 이해하는 기술\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM1cxOUrVtnX"
      },
      "source": [
        "## 자연어 처리 task\n",
        "- 모델의 특정 언어 이해 능력을 평가하는데 사용 가능\n",
        "  - example\n",
        "  - named entity recognition: 문장 내 특정 단어가 고유 명사, 기관명 혹은 엔티티인가?\n",
        "  - taxtual entailment: 두 문장이 주어졌을 때, 첫 문장이 두 번째 문장을 수반하는가 위배하는가\n",
        "\n",
        "  - BERT\n",
        "    - 고객이 남긴 리뷰에 대한 감정 예측\n",
        "    - 이 경우, 모델이 __coreference resolution__에 어떤 성능을 보이는지에 대해 관심 X\n",
        "    - 그러나 자연어 처리 task들은 언어적 측면에서 서로 연결되어 있다는 점을 이해하는 것이 중요\n",
        "      - 만약 감정분석에만 관심이 있더라도, 문장 내 대명사가 가리키는 바가 무엇인지를 잘 아는 모델들\n",
        "      - 즉 Coreference resolution을 해결할 수 있는 모델들은 고객 리뷰 내 모호한 명사들  효과적으로 판단\n",
        "    - 전이학습 모델들이 GLUE 같이 다양한 자연어 처리 태스크에서 좋은 성능을 보인다는 것은 해당 모델들은 “특정” 태스크에도 효과적으로 적용될 수 있음 의미\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXwVg85X-pZ"
      },
      "source": [
        "## GLUE의 탄생배경\n",
        "- 과거 자연어 처리 모델들은 거의 대부분 하나의 특정 문제를 잘 해결하기 위해 설계\n",
        "- end-to-end로 해당 문제를 푸는데만 적합하게 훈련된 과거 모델들은 다른 문제나 다른 데이터셋에 대해 효과적X\n",
        "- 특정 문제 혹은 심지어 특정 데이터셋을 염두에 두고 설계되었기 때문에 모델이 잘 훈련되었는지 확인하는 것은 굉장히 쉬운 일\n",
        "\n",
        "- __전이학습(Transfer Learning)__의 성공이 이어지며 모델을 평가하기 위한 새로운 방법론의 필요성 대두\n",
        "- 특정 문제만을 해결하기 위해 End-to-end 방식으로 학습된 Single task model들과 달리, 전이학습 모델들은 Deep한 모델을 이용해 자연어의 일반화된 이해를 중점으로 학습\n",
        "- 즉, 전이학습 모델들은 사전학습을 통해 언어에 대한 일반적인 이해 능력 가짐\n",
        "- 사전학습을 통해 얻어진 자연어 이해 능력은 해당 모델을 특정 태스크를 수행하기 위해 __Fine-Tuning__ 할 때 그 빛을 발함\n",
        "\n",
        "  - 사전학습에 사용되었던 입력층과 출력층을 기존 모델에서 제거\n",
        "  - 입력층과 출력층을 해결하고자 하는 문제에 적합한 층으로 교체\n",
        "  - 위 과정을 거쳐 변형된 모델을 n번의 에폭 동안 재학습(Fine-Tuning)\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umIFUktIGlWJ"
      },
      "source": [
        "## GLUE 탄생과 활용 예\n",
        "- 연구자들은 자신이 새로이 연구한 모델을 GLUE 데이터셋에 훈련시킨 후, GLUE 내 9개의 태스크에 각각 점수를 메겨 최종 성능 점수를 계산할 수 있게 됨\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDi1MvE5JI4n"
      },
      "source": [
        "## GLUE 데이터에 모델을 테스트하는 방법\n",
        "- BERT는 기본적으로 두 문장을 입력으로 취할 수 있도록 설계(두 번째 문장의 입력은 선택의 문제)되었기 때문에 GLUE 데이터셋에 대해 훈련을 진행할 때에 입력 구조를 바꿀 필요가 없다.\n",
        "  - 두 입력 문장은 QQP 태스크를 수행하기 위한 두 질문으로 대체될 수 있고, CoLA 태스크를 수행하기 위해 하나의 문장만 입력으로 넣을 수도 있으며, QNLI 태스크를 수행하기 위해 질문과 문단을 각각의 문장 입력 값으로 넣을 수도 있게 되는 것입니다.\n",
        "\n",
        "- 태스크의 수행을 위해서 사전훈련에 사용되었던 분류층을 제거하고, 이를 GLUE 태스크를 수행하기 위한 레이어로 변경해주어야 함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdRWW3HyNmxa"
      },
      "source": [
        "# Corpus of Linguistic Acceptability (CoLA)\n",
        "<img src=\"https://inmoonlight.github.io/assets/images/cola_dataset_description.png?style=centerme\" width=\"500px\" height=\"150px\"></img>\n",
        "- included\n",
        "  - (a) Morphological Violation: \"should leave\" 가 올바른 표현이지만 \"should leaving\"으로 작성되었다. 동사의 형태(verbal inflection)가 맞지 않는 경우에 해당한다. (b) Syntactic Violation: \"What did Bill buy?\" 혹은 \"Bill bought potatoes and _\" 이 되어야 한다. 통사 구조가 틀린 경우에 해당한다. (c) Semantic Violation: 의미적으로 말이 되지 않는 문장에 해당한다.\n",
        "- excluded\n",
        "  - (d) Pragmatic Anomalies: grammar와 상관없는 외부 지식이 필요하므로 제외되었다. (e) Unavailable Meanings: 문장만보고는 판단이 애매하므로 제외되었다. (f) Prescriptive Rules: 사람도 누군가의 가르침없이는 터득하기 어려운 rule이기 때문에 제외되었다. (g) Nonce Words: \"arrivable\"과 같이 typical word-level NLP 모델의 vocab에는 등장하지 않는 단어가 포함된 경우이다. NLP 모델의 scope이 아니라고 판단되어 제외되었다.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLJNYTG2fN3L"
      },
      "source": [
        "- baseline: BiLSTM\n",
        "- dataset (9)\n",
        "  - single-sentence tasks\n",
        "    - CoLA (Corpus of Linguistic Acceptability)\n",
        "    - SST-2 (Stanford Sentiment Treebank)\n",
        "  - similarity and paraphrase tasks\n",
        "    - MRPC (Microsoft Research Paraphrase Corpus)\n",
        "    - QQP (Quora Question Pairs)\n",
        "    - STS-B (Semantic Textual Similarity Benchmark)\n",
        "  - inference tasks\n",
        "    - MNLI (Multi-Genre Natural Language Inference)\n",
        "    - QNLI (Question Natural Language Inference)\n",
        "    - RTE (Recognizing Textual Entailment)\n",
        "    - WNLI (Winograd Natural Language Inference)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAv2cSm5h0BV"
      },
      "source": [
        "## superGLUE\n",
        "- GLUE보다 더 어려운 NLU 과제를 모은 벤치마크\n",
        "- baseline : BERT\n",
        "- dataset (8)\n",
        "  - BoolQ (Boolean Questions)\n",
        "  - CB (CommitmentBank)\n",
        "  - COPA (Choice of Plausible Alternatives)\n",
        "  - MultiRC (Multi-Sentence Reading Comprehension)\n",
        "  - ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)\n",
        "  - RTE (Recognizing Textual Entailment)\n",
        "  - WiC (Word-in-Context)\n",
        "  - WSC (Winograd Schema Challenge)\n",
        "---"
      ]
    }
  ]
}