{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paper_GLUE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYXvlMQ9e3RVI1ImIfHpyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/INU-DILAB/blob/main/Korean_corpus/paper_GLUE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxwz47-07J3f"
      },
      "source": [
        "# [paper review] GLUE: a multi-task benchmark and analysis platform for natural language understanding\n",
        "- 2019\n",
        "<https://openreview.net/pdf?id=rJ4km2R5t7>   \n",
        "[예시](https://doc.mindscale.kr/km/BERT.pdf)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5fNH4ZpBqGA"
      },
      "source": [
        "## Abstract\n",
        "- 자연어 이해 기술(NLU) 기술이 최대한 유용하려면 single task, 장르, 데이터셋에 배타적이지 않은 방식으로 언어를 처리 가능해야 함\n",
        "- 이를 위해 GLUE banchmark (General Language Understanding Evaluation)를 사용\n",
        "  - **GLUE banchmark**: 다양한 기존 NLU 전반에 걸쳐 모델의 성능을 평가하기 위한 도구 모음\n",
        "  - 제한된 training data를 가진 task를 포함함으로써, task 전반에 걸쳐 일반적인 언어 지식을 공유하는 모델을 위해 설계\n",
        "  - 모델에 대한 자세한 언어 분석이 가능한 수작업 진단 테스트들 포함\n",
        "- 논문은 transfer and representation 학습을 위한 현재 방법을 기반으로하는 baseline을 평가\n",
        "- 모든 작업에 대한 muti-task training이 별도의 모델을 학습시키는 것보다 더 잘 수행됨을 발견\n",
        "- 그러나 우리의 best model의 절대 성능이 낮다는 것은 일반 NLU 시스템의 개선 필요성을 나타냄"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQe85NBTG-i9"
      },
      "source": [
        "---\n",
        "## 1. Introduction\n",
        "- 대부분의 NLU 모델은 특정 작업 위해 설계\n",
        "- 다양한 언어적 작업을 수행하는 보다 통일된 모델을 개발하는 것 중요\n",
        "  - 이런 연구를 위해, GLUE benchmark 사용\n",
        "  - **GLUE**: 질문 답변, 감정 분석, 텍스트 함의(entailment)를 포함한 NLU 작업의 모음과 모델 평가, 비교 분석을 위한 플랫폼\n",
        "  - 단일 문장과 문장 쌍 입력을 처리\n",
        "  - GLUE는 sample-efficient 학습과 task 전반에 걸쳐 knowledge-transfer를 용이하게 하는 언어 지식 표현 방법을 배울 수 있는 모델 선호\n",
        "\n",
        "- GLUE가 제기하는 문제를 잘 이해하기 위해, 논문은 간단한 baseline과 최신 sentence representation 모델을 사용하여 실험.\n",
        "- 논문은 multi-task trained model이 각 task에 대해 개별적으로 훈련된 유사한 모델을 약간 능가한다는 것을 발견.\n",
        "- 논문의 best multi-task model은 pre-training 기술인 ELMo를 사용\n",
        "  - ELMo는 여전히 낮은 absolute score.\n",
        "- diagnostic dataset(진단 데이터셋) 사용 분석 결과, baseline 모델은 강력한 어휘적 신호를 잘 처리하지만 깊은 논리적 구조에는 어려움을 겪음.\n",
        "\n",
        "- 요약 we offer:\n",
        "  1. 논문은 확립된 annotated dataset을 기반으로 구축, 다양한 텍스트 장르, 데이터셋 크기, 난이도를 다루기 위해 선택된 9개의 [문장/ 문장 쌍] NLU tasks 제공\n",
        "  2. 주로 private test data를 기반으로 하는 온라인 평가 플랫폼, 리더보드는 모델에 구애받지 않으며 9개 task 모두에 대해 결과를 생성할 수 있는 모든 방법 평가 가능\n",
        "  3. 전문가에 의해 구성된 진단 평가 데이터셋\n",
        "  4. 문장 표현 학습에 대한 몇 가지 주요 접근법에 대한 baseline 결과\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDhaOk6mQ0GC"
      },
      "source": [
        "---\n",
        "## 2. Related work\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vxQiE4oVYhE"
      },
      "source": [
        "## 3. Tasks\n",
        "- GLUE는 광범위한 영역, 데이터 양 같은 어려움을 다루는 9개의 영어 문장 이해가 중점\n",
        "- 목표 \n",
        "  - 일반화 가능한 NLU 시스템 개발\n",
        "  - 우수한 성능을 위해 일부 task별 구성 요소를 유지하면서, 모든 task에 걸쳐 상당한 지식(ex.훈련된 매개변수)을 공유하는 모델 요구하는 banchmark 설계\n",
        "- **table1** 에서 과제 설명\n",
        "  - STS-B를 제외한 모두: 단일 문장 or 문장 쌍 분류\n",
        "  - MNLI: 3 class / 나머지는 2 class\n",
        "  - bold체 test set: 라벨 공개 X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVw0whr0XMvF"
      },
      "source": [
        "### 3.1 Single-Sentence Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaRyW-SiX39D"
      },
      "source": [
        "#### CoLA\n",
        "- The Corpus of Linguistric Acceptability(Warstadt..2018) - 문법성 용인 말뭉치\n",
        "- 문법적으로 올바른 문장을 고를 수 있는 것은 언어 능력의 핵심\n",
        "- 신경망이 문법적으로 올바른 문장을 구분할 수 있는지 테스트하기 위한 데이터셋\n",
        "- 언어 이론에 관한 책과 기사\n",
        "\n",
        "- 논문은 불균형 이진 분류의 성능을 평가하는 지표로 Matthews correlation coefficient(Matthews, 1975) 사용, 범위는 -1 ~ 1, 0은 정보 없는 추측 성능\n",
        "- 테스트셋의 도메인 내 섹션과 외 섹션의 조합에 대한 single performance number을 보고\n",
        "\n",
        "- 예시\n",
        "  - The more books I ask to whom he will give, the more he reads. --> F\n",
        "  - I said that my father, he was tight as a hoot-owl. --> T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OITJfvH8aax-"
      },
      "source": [
        "#### SST-2\n",
        "- The Stanford Sentiment Treebank (Socher..2013) - 감정 분석\n",
        "- 영화 리뷰 문장과 감정 주석\n",
        "- 과제는 주어진 문장의 감정을 예측하는 것\n",
        "- 우리는 양방향(긍정/부정) 클래스 분할, 문장 수준 레이블만 사용\n",
        "- 예시\n",
        "  - 이 영화를 용서할 수 없다. --> 부정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBfETmc6dfvw"
      },
      "source": [
        "### Similarity and Paraphrase Tasks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wQ37pKddw4Q"
      },
      "source": [
        "#### MRPC\n",
        "- The Microsoft Research Paraphrase Corpus (Dolan,,,2005)\n",
        "- 뉴스 소스에서 추출된 문장 쌍의 말뭉치, 쌍에 대한 주석\n",
        "- 정확도와 F1 score 보고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLB4Rv-Ne3D9"
      },
      "source": [
        "#### QQP\n",
        "- The Quora Question Paris dataset \n",
        "- 커뮤니티 질문 사이트의 질문 쌍 모음\n",
        "- 두 질문이 semantic하게 동일 여부 확인\n",
        "- 정확도와 F1 score 보고\n",
        "- 표준 데이터셋을 사용, 우리는 test set이 train set과 다른 라벨 분포를 가지는지 관찰\n",
        "- 예시\n",
        "  - Q1: Which pizzas are the most popularly ordered pizzas on Domino's menu?\n",
        "  - Q2: How many calories does a Dominos pizza have?\n",
        "  - Label: 0\n",
        "  - Q1: How do you start a bakery? \n",
        "  - Q2: How can one start a bakerybusiness\n",
        "  - Label: 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt6APtOqfibD"
      },
      "source": [
        "#### STS-B\n",
        "- The Semantic Textual Similarity Benchmark (Cer..2017) \n",
        "- 뉴스 헤드라인, 비디오 및 이미지 캡션, 자연어 추론 데이터에서 도출된 문장 쌍 모음\n",
        "- 각 쌍은 1~5 smilarity score, 이 점수 예측하는 것이 과제\n",
        "- 일반적으로 Pearson and Spearman correlation coefficients 사용하여 평가\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9h3Vwwgsgw"
      },
      "source": [
        "### 3.3 Inference Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2a58zcgyqk"
      },
      "source": [
        "#### MNLI\n",
        "- The Multi-Genre Natural Language Inference Corpus (Williams..2018)\n",
        "- 두 문장의 관계는 중립(neutral), 모순(contradiction), 함의(entailment)로 분류\n",
        "-  가설(hypothesis)이 전제(premise)로부터 도출될 수 있는지 판단하는 과제\n",
        "- 일치 섹션(in-domain) 과 일치X 섹션(cross-domain) 모두 평가\n",
        "\n",
        "- 예시: neutral\n",
        "  - Genre: letters\n",
        "  - Premise: Your gift is appreciated by each and every student who will benefit from your generosity.\n",
        "  - Hypothesis: Hundreds of students will benefit from your generosity.\n",
        "- 예시: 모순(contradiction)\n",
        "  - Genre: Telephone Speech\n",
        "  - Premise: yes now you know if if everybody like in August when everybody's on vacation or something we can dress a little more casual or\n",
        "  - Hypothesis: August is a black outmonth for vacations in the company.\n",
        "- 예시: 함의(entailment)\n",
        "  - Genre: Report\n",
        "  - Premise: At the other end of Pennsylvania Avenue, people began to line up for a White House tour.\n",
        "  - Hypothesis: People formed a line at the end of Pennsylvania Avenue.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5k2rgij_q0Z"
      },
      "source": [
        "#### QNLI\n",
        "- The Stanford Question Answering Dataset(Rajpurkar...2016): question-paragraph 쌍으로 이루어진 질의-응답 dataset\n",
        "  - (위키피디아에서 가져온) 단락의 문장 중 하나가 질문에 대한 답 포함\n",
        "  - Paragraph이 주어지고 Question에 대해 올바른 정답을 할 수 있는지에 대한 문제(binary)\n",
        "  - 각 문항과 해당 문맥의 각 문장\n",
        " 사이에 쌍을 형성하고 문항과 문맥 문장 사이에 어휘적 중복이 낮은 쌍을 필터링하여 과제를 문장쌍 분류로 변환\n",
        "  - 과제는 문맥에 질문에 대한 답이 포함되어 있는지 여부 결정\n",
        "- 변형된 NLI(Question-answering NLI)\n",
        "  - 이 수정된 버전은 모델이 정확한 답을 선택해야 하는 요건과 답이 입력에 항상 존재하며 어휘적 중복이 신뢰할 수 있는 신호라는 단순화된 가정 제거\n",
        "  - 기존 데이터셋을 NLI로 재캐스팅\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJYMDzQCbAW"
      },
      "source": [
        "#### RTE\n",
        "- The Recognizing Textual Entailment datasets\n",
        "- series of annual textual entailment challenges에서 비롯.\n",
        "- 우리는 RTE1, RTE2, RTE3, RTE5 데이터 결합\n",
        "- 예제는 뉴스와 위키피디아 text 기반\n",
        "- 모든 데이터셋을 two-class로 분할하고, three-class datasets은 일관성을 위해 (nautral, cotradiction)을 not_entailment로 축소한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwma2fLKDyuI"
      },
      "source": [
        "#### WNLI\n",
        "- The Winograd Schema Challenge(Levesque 2017)\n",
        "- Winograd Schema를 두 문장의 관계를 판단하는 과제로 변형\n",
        "  - Winograd Schema: 문장에서 한 단어를 바꿨을 때, 변하는 대명사의 뜻 감지\n",
        "- 우리는 문제를 문장 쌍 분류로 변환하기 위해, 모호한 대명사를 가능한 각 참조어로, 대명사는 original sentence 로 대체\n",
        "- 대명사가 대체된 문장이 원래 문장에 entail되는지 예측하는 것이 과제\n",
        "- 우리는 새로운 example로 소설책에서 나온 작은 evaluation set 사용.\n",
        "- train set은 두 등급 사이에 균형을 이루지만 test set은 불균형 / 데이터 변덕 / 때때로 가설이 training과 development example을 공유하므로 model이 train example을 기억하는 경우 잘못된 label이 예측될 가능성\n",
        "- 예시\n",
        "  - Premise: The city councilmen refused the demonstrators a\n",
        "permit because they feared violence.\n",
        "  - Hypothesis: The demonstrators feared violence\n",
        "  - Label: 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSZlJFtsLFqi"
      },
      "source": [
        "### Evaluation\n",
        "- GLUE benchmark는 SemEval and Kaggle과 동일한 평가 모델\n",
        "- benchmark에서 평가하려면 제고된 test data에서 시스템을 실행한 다음 결과를 웹사이트 gluebenchmark.com에 업로드하여 점수 매김\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo3ihssnMBsC"
      },
      "source": [
        "---\n",
        "## 4. Diagnostic Dataset\n",
        "- 진단 데이터셋 - small, manually-curated test set\n",
        "- 모델이 포착하는 데 중요한 미리 정의된 phenomena(현상) set을 강조\n",
        "- **table2**: full set of phenomena (부록E)\n",
        "- 각 diagnostic example은 입증된 현상에 대한 태그가 있는 문장 쌍\n",
        "- NLI 과제는 문장 이해에 관련된 모든 기술을 쉽게 평가 가능하게 함.\n",
        "  - 다양한 언어 현상의 예를 만들고 다양한 분야의 문장을 바탕으로 데이터 다양화\n",
        "- **table 3**: dataset sample\n",
        "- annotation process\n",
        "  - entailment 42%, neutral 35%, contradiction 23% 로 맞춤\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8_WaeRLNuG_"
      },
      "source": [
        "---\n",
        "## 5. Baselines\n",
        "- 간단한 기본 구조: sentence to vector encoder\n",
        "- max pooling, 300D GloVe word embedding 있는 2-layer 1500D(방향당) BiLSTM 사용\n",
        "- pre training을 위한 ELMo, CoVe로 baseline 강화\n",
        "- CoVe는 원래 영-독 번역용으로 훈련된 2-layer BiLSTM encoder 사용, CoVe vector를 GloVe embedding에 연결"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76OfB2xSfZQS"
      },
      "source": [
        "---\n",
        "## 6. Benchmark Results\n",
        "- training runs of each model\n",
        "- evaluating the run with the best macro-average development set performance(table 6 in Appendix C)\n",
        "- **table 4**: 주요 benchmark 작업에 대한 성과\n",
        "  - single-task and sentence representation model은 각 독립적인 task에 대해 best run 평가함.\n",
        "    - multi-task training은 Attention or ELMo 사용 모델 중에서 single-task 보다 전반적으로 우수한 점수???\n",
        "- Attention: 일반적으로 single에서는 부정적인 효과 / multi에서는 도움\n",
        "  - sigle-sentence(CoLA, SST-2) tasks에서 CoVe 대신 ELMo 임베딩을 사용하는 것이 좋다.\n",
        "  - pre-trained sentence model에서 CBoW-----> GenSen으로 갈수록 상당히 이득\n",
        "  - STS-B, 직접 훈련된 모델은 best sentence representation model 에 비해 크게 성능 안좋음\n",
        "  -  WNLI는 어떠한 모델도 특별히 잘하지 못함\n",
        "- best baselines을 개선할 여지 존재\n",
        "- 결과, solving GLUE는 현재 모델과 방법의 능력을 벗아난다는 것 나타냄.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6AoA0R6qQ_t"
      },
      "source": [
        "## 7. Analysis\n",
        "- MNLI classifier를 평가함으로써 baseline 분석 **table 5**\n",
        "- coarse categories\n",
        "  - 모든 모델에서 전반적으로 성능이 낮다.\n",
        "  - 성능은 predicate-argument(술어-주장)구조에서 더 높고 logic에서 더 낮은 경향\n",
        "  - multi-tasking model은 거의 항상 single보다 성능 우수\n",
        "  - GLUE 작업에 대해 훈련된 모델은 GenSen을 제외하고 pre-trainde sentence 모델보다 휠씬 우수한 성능\n",
        "  - Attention은 ELMo, CoVe보다 diagnostic score에 더 큰 영향 미침\n",
        "    - NLI 일반화에 Attention이 특히 중요\n",
        "- fine-grained subcategories\n",
        "  - 대부분의 모델은 universal quantification 비교적 잘 처리\n",
        "  - lexical cues에 의존하는 것만으로도 양호한 성능 발휘\n",
        "    - morphological negation 예제에서 좋은 신호를 제공\n",
        "  - 약점\n",
        "    - double negation은 GloVe 임베딩만 사용하는 GLUE train model 에서 특히 어려움. -> ELMo, CoVe로 개선\n",
        "    - Attention이 있으면 downward monotonicity에 어려움.\n",
        "    - 모델들이 entailment의 신호로써, hypernym/hyponym substitution과 word deletion에 민감하지만 upwoad monotonicity 문맥인 것처럼 잘못 예측.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaC2Wj-71N-A"
      },
      "source": [
        "## 8. Conclusion\n",
        "- 공동으로 훈련된 모델이 개별적으로 훈련된 모델보다 전반적으로 성능이 좋다.\n",
        "- 우리는 GLUE benchmark에서 best sentence representation model을 능가하기 위해 NLU의 ELMo와 같은 Attention 구조와 transfer 방법이 괜찮다는 것을 알았으나 여전히 개선할 여지가 있음.\n",
        "- diagnostic data set에서 모델을 평가할 때, 많은 언어 현상에 대해 실패하여 향후 작업에 대한 가능성 시사\n",
        "- NLU 모델을 여전히 어떻게 설계할 것인가 의문이 남음\n",
        "---\n"
      ]
    }
  ]
}
